[{"permalink":"/notebooks-101/","layout":"default","title":null,"content":"<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3478666.svg)](https://doi.org/10.5281/zenodo.3478666)\n\n[![Documentation Status](https://readthedocs.org/projects/comet-notebooks-101/badge/?version=latest)](https://comet-notebooks-101.readthedocs.io/)\n\n# notebooks-101\nEverything you need to know about running Jupyter Notebooks on HPC Systems at SDSC.\nRead the tutorials here: https://comet-notebooks-101.readthedocs.io/\n\n## Interactive Video Link:\nThis material was presented as part of a webinar presented in April, 2020. The link to the traiing material can be found here:\n* https://github.com/sdsc-hpc-training-org/comet-101/blob/master/running_jobs_on_comet.md\nAnd the interactive video can be found here:\n* https://education.sdsc.edu/training/interactive/202005_running_jupyter_notebooks_on_comet/index.php\n\n## GitHub Repo to training material:\nhttps://github.com/sdsc-hpc-training/basic_skills/tree/master/how_to_run_notebooks_on_comet\n\n\n## License\n\nAll the teaching material in this repository is licensed under [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n","dir":"/notebooks-101/","name":"README.md","path":"notebooks-101/README.md","url":"/notebooks-101/"},{"permalink":"/comet-101/","layout":"default","title":"Comet 101 Tutorial","content":"# Comet 101 Tutorial\n\n## NOTES:\n\n## Interactive Video Link:\nThis material was presented as part of a webinar presented on xxxxx.\nThe link to the presentation can be found here:\n\nhttps://education.sdsc.edu/training/interactive/202004_intro_to_comet/index.php\n\n## Hands-on Self-guided Tutorial:\n\nhttps://github.com/sdsc-hpc-training-org/comet-101/blob/master/running_jobs_on_comet.md\n\n## Link to GitHub Repo\nhttps://github.com/sdsc-hpc-training-org/webinars/tree/master/202004_comet_101\n\n### Comet User Guide\nPlease read the Comet user guide and familiarize yourself with the hardware, file systems, batch job submission, compilers and modules. The guide can be found here:\n\nhttp://www.sdsc.edu/support/user_guides/comet.html\n\n### Basic Skills\nYou must be familiar with running basic Unix commands, connecting to Comet via SSH, and other basic skills. See:\nhttps://github.com/sdsc-hpc-training/basic_skills\n\n","dir":"/comet-101/","name":"README.md","path":"comet-101/README.md","url":"/comet-101/"},{"layout":"default","title":"Comet 101: Introduction to Running Jobs on Comet Supercomputer","content":"# Comet 101: Introduction to Running Jobs on Comet Supercomputer\n\nPresented by Mary Thomas (SDSC, <mpthomas@ucsd.edu> )\n<hr>\nIn this tutorial, you will learn how to compile and run jobs on Comet, where to run them, and how to run batch jobs.\nThe commands below can be cut & pasted into the terminal window, which is connected to comet.sdsc.edu. For instructions on how to do this, see the tutorial on how to use a terminal application and SSH go connect to an SDSC HPC system: https://github.com/sdsc-hpc-training/basic_skills/tree/master/connecting_to_hpc_systems.\n\n\n# Misc Notes/Updates:\n* You must have a comet account in order to access the system.\n * To obtain a trial account: http://www.sdsc.edu/support/user_guides/comet.html#trial_accounts\n* You must be familiar with running basic Unix commands: see the following tutorials at:\n * https://github.com/sdsc-hpc-training/basic_skills\n* The `hostname` for Comet is `comet.sdsc.edu`\n* The operating system for Comet was changed to CentOS in December, 2019. As a result, you will need to recompile all code, some modules and libraries are no longer needed, and the locations of some libraries and applications have changed. For details, see the transition guide here:\n * https://www.sdsc.edu/services/hpc/comet_upgrade.html\n* Our next HPC system, [E X P A N S E](https://expanse.sdsc.edu), will be coming online for early users in September. Keep an eye on the E X P A N S E pages for training information and other updates\n\n<em>If you have any difficulties completing these tasks, please contact SDSC Consulting group at <consult@sdsc.edu>.</em>\n<hr>\n\n<a name=\"top\">Contents:\n* [Comet Overview](#overview)\n * [Comet Architecture](#network-arch)\n * [Comet File Systems](#file-systems)\n\n* [Getting Started - Comet System Environment](#sys-env)\n * [Comet Accounts](#comet-accounts)\n * [Logging Onto Comet](#comet-logon)\n * [Obtaining Example Code](#example-code)\n\n* [Modules: Managing User Environments](#modules)\n * [Common module commands](#module-commands)\n * [Load and Check Modules and Environment](#load-and-check-module-env)\n * [Module Error: command not found](#module-error)\n\n* [Compiling & Linking](#compilers)\n * [Supported Compiler Types](#compilers-supported)\n * [Using the Intel Compilers](#compilers-intel)\n * [Using the PGI Compilers](#compilers-pgi)\n * [Using the GNU Compilers](#compilers-gnu)\n\n* [Running Jobs on Comet](#running-jobs)\n * [The SLURM Resource Manager](#running-jobs-slurm)\n * [Common Slurm Commands](#running-jobs-slurm-commands)\n * [Slurm Partitions](#running-jobs-slurm-partitions)\n * [Interactive Jobs using SLURM](#running-jobs-slurm-interactive)\n * [Batch Jobs using SLURM](#running-jobs-slurm-batch-submit)\n * [Command Line Jobs](#running-jobs-cmdline)\n\n* [Hands-on Examples](#hands-on)\n* [Compiling and Running GPU/CUDA Jobs](#comp-and-run-cuda-jobs)\n * [GPU Hello World (GPU) ](#hello-world-gpu)\n * [GPU Hello World: Compiling](#hello-world-gpu-compile)\n * [GPU Hello World: Batch Script Submission](#hello-world-gpu-batch-submit)\n * [GPU Hello World: Batch Job Output](#hello-world-gpu-batch-output)\n * [GPU Enumeration ](#enum-gpu)\n * [GPU Enumeration: Compiling](#enum-gpu-compile)\n * [GPU Enumeration: Batch Script Submission](#enum-gpu-batch-submit)\n * [GPU Enumeration: Batch Job Output](#enum-gpu-batch-output)\n * [CUDA Mat-Mult](#mat-mul-gpu)\n * [Matrix Mult. (GPU): Compiling](#mat-mul-gpu-compile)\n * [Matrix Mult. (GPU): Batch Script Submission](#mat-mul-gpu-batch-submit)\n * [Matrix Mult. (GPU): Batch Job Output](#mat-mul-gpu-batch-output)\n\n* [Compiling and Running CPU Jobs](#comp-and-run-cpu-jobs)\n * [Hello World (MPI)](#hello-world-mpi)\n * [Hello World (MPI): Source Code](#hello-world-mpi-source)\n * [Hello World (MPI): Compiling](#hello-world-mpi-compile)\n * [Hello World (MPI): Interactive Jobs](#hello-world-mpi-interactive)\n * [Hello World (MPI): Batch Script Submission](#hello-world-mpi-batch-submit)\n * [Hello World (MPI): Batch Script Output](#hello-world-mpi-batch-output)\n * [Hello World (OpenMP)](#hello-world-omp)\n * [Hello World (OpenMP): Source Code](#hello-world-omp-source)\n * [Hello World (OpenMP): Compiling](#hello-world-omp-compile)\n * [Hello World (OpenMP): Batch Script Submission](#hello-world-omp-batch-submit)\n * [Hello World (OpenMP): Batch Script Output](#hello-world-omp-batch-output)\n * [Compiling and Running Hybrid (MPI + OpenMP) Jobs](#hybrid-mpi-omp)\n * [Hybrid (MPI + OpenMP): Source Code](#hybrid-mpi-omp-source)\n * [Hybrid (MPI + OpenMP): Compiling](#hybrid-mpi-omp-compile)\n * [Hybrid (MPI + OpenMP): Batch Script Submission](#hybrid-mpi-omp-batch-submit)\n * [Hybrid (MPI + OpenMP): Batch Script Output](#hybrid-mpi-omp-batch-output)\n\n[Back to Top](#top)\n<hr>\n\n## <a name=\"overview\"></a>Comet Overview:\n\n### HPC for the \"long tail of science:\"\n* Designed and operated on the principle that the majority of computational research is performed at modest scale: large number jobs that run for less than 48 hours, but can be computationally intensvie and generate large amounts of data.\n* An NSF-funded system available through the eXtreme Science and Engineering Discovery Environment (XSEDE) program.\n* Also supports science gateways.\n\n<img src=\"images/comet-rack.png\" alt=\"Comet Rack View\" width=\"500px\" />\n* 2.76 Pflop/s peak\n* 48,784 CPU cores\n* 288 NVIDIA GPUs\n* 247 TB total memory\n* 634 TB total flash memory\n\n\n<img src=\"images/comet-characteristics.png\" alt=\"Comet System Characteristics\" width=\"500px\" />\n\n[Back to Top](#top)\n<hr>\n\n<a name=\"network-arch\"></a><img src=\"images/comet-network-arch.png\" alt=\"Comet Network Architecture\" width=\"500px\" />\n\n[Back to Top](#top)\n<hr>\n\n<a name=\"file-systems\"></a><img src=\"images/comet-file-systems.png\" alt=\"Comet File Systems\" width=\"500px\" />\n* Lustre filesystems – Good for scalable large block I/O\n * Accessible from all compute and GPU nodes.\n * /oasis/scratch/comet - 2.5PB, peak performance: 100GB/s. Good location for storing large scale scratch data during a job.\n * /oasis/projects/nsf - 2.5PB, peak performance: 100 GB/s. Long term storage.\n * *Not good for lots of small files or small block I/O.*\n\n* SSD filesystems\n * /scratch local to each native compute node – 210GB on regular compute nodes, 285GB on GPU, large memory nodes, 1.4TB on selected compute nodes.\n * SSD location is good for writing small files and temporary scratch files. Purged at the end of a job.\n\n* Home directories (/home/$USER)\n * Source trees, binaries, and small input files.\n * *Not good for large scale I/O.*\n\n\n[Back to Top](#top)\n<hr>\n\n\n## <a name=\"sys-env\"></a>Getting Started on Comet\n\n### <a name=\"comet-accounts\"></a>Comet Accounts\nYou must have a comet account in order to access the system.\n* Obtain a trial account here: http://www.sdsc.edu/support/user_guides/comet.html#trial_accounts\n* You can use your XSEDE account.\n\n### <a name=\"comet-logon\"></a>Logging Onto Comet\nDetails about how to access Comet under different circumstances are described in the Comet User Guide:\n http://www.sdsc.edu/support/user_guides/comet.html#access\n\nFor instructions on how to use SSH, see [here](https://github.com/sdsc/sdsc-summer-institute-2020/tree/master/0_preparation/connecting-to-hpc-systems)\n```\n[mthomas@gidget:~] ssh -Y comet.sdsc.edu\nPassword:\nLast login: Fri Jul 31 14:20:40 2020 from 76.176.117.51\nRocks 7.0 (Manzanita)\nProfile built 12:32 03-Dec-2019\n\nKickstarted 13:47 03-Dec-2019\n\n WELCOME TO\n __________________ __ _______________\n -----/ ____/ __ \\/ |/ / ____/_ __/\n --/ / / / / / /|_/ / __/ / /\n / /___/ /_/ / / / / /___ / /\n \\____/\\____/_/ /_/_____/ /_/\n###############################################################################\nNOTICE:\nThe Comet login nodes are not to be used for running processing tasks.\nThis includes running Jupyter notebooks and the like. All processing\njobs should be submitted as jobs to the batch scheduler. If you don’t\nknow how to do that see the Comet user guide\nhttps://www.sdsc.edu/support/user_guides/comet.html#running.\nAny tasks found running on the login nodes in violation of this policy\n may be terminated immediately and the responsible user locked out of\nthe system until they contact user services.\n###############################################################################\n(base) [mthomas@comet-ln2:~]\n\n```\n\n[Back to Top](#top)\n<hr>\n\n### <a name=\"example-code\"></a>Obtaining Example Code\n* Create a test directory hold the comet example files:\n```\n[comet-ln2 ~]$ mkdir comet-examples\n[comet-ln2 ~]$ ls -al\ntotal 166\ndrwxr-x--- 8 user user300 24 Jul 17 20:20 .\ndrwxr-xr-x 139 root root 0 Jul 17 20:17 ..\n-rw-r--r-- 1 user use300 2487 Jun 23 2017 .alias\n-rw------- 1 user use300 14247 Jul 17 12:11 .bash_history\n-rw-r--r-- 1 user use300 18 Jun 19 2017 .bash_logout\n-rw-r--r-- 1 user use300 176 Jun 19 2017 .bash_profile\n-rw-r--r-- 1 user use300 159 Jul 17 18:24 .bashrc\ndrwxr-xr-x 2 user use300 2 Jul 17 20:20 comet-examples\n[snip extra lines]\n[comet-ln2 ~]$ cd comet-examples/\n[comet-ln2 comet-examples]$ pwd\n/home/user/comet-examples\n[comet-ln2 comet-examples]$\n```\n* Copy the `/share/apps/examples/comet101/` directory to your local (`/home/username/comet-examples`) directory. Note: you can learn to create and modify directories as part of the *Getting Started* and *Basic Skills* preparation work:\nhttps://github.com/sdsc/sdsc-summer-institute-2020/tree/master/0_preparation\n```\n[mthomas@comet-ln3 ~]$ ls -al /share/apps/examples/hpc-training/comet-examples/\ntotal 20\n(base) [mthomas@comet-ln2:~/comet101] ll /share/apps/examples/hpc-training/comet101/\ntotal 32\ndrwxr-sr-x 8 mthomas use300 4096 Apr 16 10:39 .\ndrwxrwsr-x 4 mahidhar use300 4096 Apr 15 23:37 ..\ndrwxr-sr-x 5 mthomas use300 4096 Apr 16 03:30 CUDA\ndrwxr-sr-x 2 mthomas use300 4096 Apr 16 10:39 HYBRID\ndrwxr-sr-x 2 mthomas use300 4096 Apr 16 10:39 jupyter_notebooks\ndrwxr-sr-x 2 mthomas use300 4096 Apr 16 16:46 MKL\ndrwxr-sr-x 4 mthomas use300 4096 Apr 16 03:30 MPI\ndrwxr-sr-x 2 mthomas use300 4096 Apr 16 03:31 OPENMP\n```\nCopy the 'comet101' directory into your `comet-examples` directory:\n```\n[mthomas@comet-ln3 ~]$\n[mthomas@comet-ln3 ~]$ cp -r /share/apps/examples/comet101/ comet-examples/\n[mthomas@comet-ln3 ~]$ ls -al comet-examples/\ntotal 105\ndrwxr-xr-x 5 username use300 6 Aug 5 19:02 .\ndrwxr-x--- 10 username use300 27 Aug 5 17:59 ..\ndrwxr-xr-x 16 username use300 16 Aug 5 19:02 comet101\n[mthomas@comet-ln3 comet-examples]$ ls -al\ntotal 132\ntotal 170\ndrwxr-xr-x 8 mthomas use300 8 Aug 3 01:19 .\ndrwxr-x--- 64 mthomas use300 98 Aug 3 01:19 ..\ndrwxr-xr-x 5 mthomas use300 5 Aug 3 01:19 CUDA\ndrwxr-xr-x 2 mthomas use300 6 Aug 3 01:19 HYBRID\ndrwxr-xr-x 2 mthomas use300 3 Aug 3 01:19 jupyter_notebooks\ndrwxr-xr-x 2 mthomas use300 6 Aug 3 01:19 MKL\ndrwxr-xr-x 4 mthomas use300 9 Aug 3 01:19 MPI\ndrwxr-xr-x 2 mthomas use300 9 Aug 3 01:19 OPENMP\n```\nMost examples will contain source code, along with a batch script example so you can run the example, and compilation examples (e.g. see the MKL example).\n\n[Back to Top](#top)\n<hr>\n\n\n## <a name=\"modules\"></a>Modules: Customizing Your User Environment\nThe Environment Modules package provides for dynamic modification of your shell environment. Module commands set, change, or delete environment variables, typically in support of a particular application. They also let the user choose between different versions of the same software or different combinations of related codes. See:\nhttp://www.sdsc.edu/support/user_guides/comet.html#modules\n\n### <a name=\"module-commands\"></a>Common module commands\n\n\n Here are some common module commands and their descriptions:\n\n| Command | Description |\n|---|---|\n| module list | List the modules that are currently loaded|\n| module avail | List the modules that are available|\n| module display <module_name> | Show the environment variables used by <module name> and how they are affected|\n| module show <module_name> | Same as display|\n| module unload <module name> | Remove <module name> from the environment|\n| module load <module name> | Load <module name> into the environment|\n| module swap <module one> <module two> | Replace <module one> with <module two> in the environment|\n\n<b> A few module commands:</b>\n\n* Default environment: `list`, `li`\n```\n[mthomas@comet-ln3:~] module list\nCurrently Loaded Modulefiles:\n 1) intel/2018.1.163 2) mvapich2_ib/2.3.2\n```\n* List available modules: `available`, `avail`, `av`\n\n```\n$ module av\n[mthomas@comet-ln3:~] module av\n\n------------------------- /opt/modulefiles/mpi/.intel --------------------------\nmvapich2_gdr/2.3.2(default)\n[snip]\n\n------------------------ /opt/modulefiles/applications -------------------------\nabaqus/6.11.2 lapack/3.8.0(default)\nabaqus/6.14.1(default) mafft/7.427(default)\nabinit/8.10.2(default) matlab/2019b(default)\nabyss/2.2.3(default) matt/1.00(default)\namber/18(default) migrate/3.6.11(default)\n. . .\neos/3.7.1(default) spark/1.2.0\nglobus/6.0 spark/1.5.2(default)\n. . .\n```\n\n[Back to Top](#top)\n<hr>\n\n### <a name=\"load-and-check-module-env\"></a>Load and Check Modules and Environment\n\n* Load modules:\n```\n[mthomas@comet-ln3:~] module list\nCurrently Loaded Modulefiles:\n 1) intel/2018.1.163 2) mvapich2_ib/2.3.2\n[mthomas@comet-ln3:~] module add spark/1.2.0\n[mthomas@comet-ln3:~] module list\nCurrently Loaded Modulefiles:\n 1) intel/2018.1.163 3) hadoop/2.6.0\n 2) mvapich2_ib/2.3.2 4) spark/1.2.0\n\n```\n\nShow loaded module details:\n```\n$ module show fftw/3.3.4\n[mthomas@comet-ln3:~] module show spark/1.2.0\n-------------------------------------------------------------------\n/opt/modulefiles/applications/spark/1.2.0:\n\nmodule-whatis\t Spark\nmodule-whatis\t Version: 1.2.0\nmodule\t\t load hadoop/2.6.0\nprepend-path\t PATH /opt/spark/1.2.0/bin\nsetenv\t\t SPARK_HOME /opt/spark/1.2.0\n-------------------------------------------------------------------\n```\n\nOnce you have loaded the modules, you can check the system variables that are available for you to use.\n* To see all variable, run the <b>`env`</b> command. Typically, you will see more than 60 lines containing information such as your login name, shell, your home directory:\n```\n[mthomas@comet-ln3 IBRUN]$ env\nSPARK_HOME=/opt/spark/1.2.0\nHOSTNAME=comet-ln3.sdsc.edu\nINTEL_LICENSE_FILE=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/licenses:/opt/intel/licenses:/root/intel/licenses\nSHELL=/bin/bash\nUSER=mthomas\nPATH=/opt/spark/1.2.0/bin:/opt/hadoop/2.6.0/sbin:/opt/hadoop/contrib/myHadoop/bin:/opt/hadoop/2.6.0/bin:/home/mthomas/miniconda3/bin:/home/mthomas/miniconda3/condabin:/opt/mvapich2/intel/ib/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/sdsc/bin:/opt/sdsc/sbin:/opt/ibutils/bin:/opt/pdsh/bin:/opt/rocks/bin:/opt/rocks/sbin:/home/mthomas/bin\nPWD=/home/mthomas\nLOADEDMODULES=intel/2018.1.163:mvapich2_ib/2.3.2:hadoop/2.6.0:spark/1.2.0\nJUPYTER_CONFIG_DIR=/home/mthomas/.jupyter\nMPIHOME=/opt/mvapich2/intel/ib\nMODULESHOME=/usr/share/Modules\nMKL_ROOT=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl\n```\n\nTo see the value for any of these variables, use the `echo` command:\n```\n[mthomas@comet-ln3 IBRUN]$ echo $PATH\nPATH=/opt/gnu/gcc/bin:/opt/gnu/bin:/opt/mvapich2/intel/ib/bin:/opt/intel/composer_xe_2013_sp1.2.144/bin/intel64:/opt/intel/composer_xe_2013_sp1.2.144/mpirt/bin/intel64:/opt/intel/composer_xe_2013_sp1.2.144/debugger/gdb/intel64_mic/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/ibutils/bin:/usr/java/latest/bin:/opt/pdsh/bin:/opt/rocks/bin:/opt/rocks/sbin:/opt/sdsc/bin:/opt/sdsc/sbin:/home/username/bin\n```\n[Back to Top](#top)\n<hr>\n\n\n### <a name=\"module-error\"></a>Troubleshooting:Module Error\n\nSometimes this error is encountered when switching from one shell to another or attempting to run the module command from within a shell script or batch job. The module command may not be inherited between the shells. To keep this from happening, execute the following command:\n```\n[comet-ln3:~]source /etc/profile.d/modules.sh\n```\nOR add this command to your shell script (including Slurm batch scripts)\n\n\n[Back to Top](#top)\n<hr>\n\n## <a name=\"compilers\"></a>Compiling & Linking\n\nComet provides the Intel, Portland Group (PGI), and GNU compilers along with multiple MPI implementations (MVAPICH2, MPICH2, OpenMPI). Most applications will achieve the best performance on Comet using the Intel compilers and MVAPICH2 and the majority of libraries installed on Comet have been built using this combination.\n\nOther compilers and versions can be installed by Comet staff on request. For more information, see the user guide:\nhttp://www.sdsc.edu/support/user_guides/comet.html#compiling\n\n### <a name=\"compilers-supported\"></a>Supported Compiler Types\n\nComet compute nodes support several parallel programming models:\n* __MPI__: Default: Intel\n * Default Intel Compiler: intel/2018.1.163; Other versions available.\n * Other options: openmpi_ib/1.8.4 (and 1.10.2), Intel MPI, mvapich2_ib/2.1\n * mvapich2_gdr: GPU direct enabled version\n* __OpenMP__: All compilers (GNU, Intel, PGI) have OpenMP flags.\n* __GPU nodes__: support CUDA, OpenACC.\n* __Hybrid modes__ are possible.\n\nIn this tutorial, we include several hands-on examples that cover many of the cases in the table:\n\n* MPI\n* OpenMP\n* HYBRID\n* GPU\n* Local scratch\n\nDefault/Suggested Compilers to used based on programming model and languages:\n\n| |Serial | MPI | OpenMP | MPI+OpenMP|\n|---|---|---|---|---|\n| Fortran | ifort | mpif90 | ifort -openmp | mpif90 -openmp |\n| C | icc | mpicc | icc -openmp | mpicc -openmp |\n| C++ | icpc | mpicxx | icpc -openmp | mpicxx -openmp |\n\n[Back to Top](#top)\n<hr>\n\n### <a name=\"compilers-intel\"></a>Using the Intel Compilers:\n\nThe Intel compilers and the MVAPICH2 MPI implementation will be loaded by default. If you have modified your environment, you can reload by executing the following commands at the Linux prompt or placing in your startup file (~/.cshrc or ~/.bashrc) or into a module load script (see above).\n```\nmodule purge\nmodule load intel mvapich2_ib\n```\nFor AVX2 support, compile with the -xHOST option. Note that -xHOST alone does not enable aggressive optimization, so compilation with -O3 is also suggested. The -fast flag invokes -xHOST, but should be avoided since it also turns on interprocedural optimization (-ipo), which may cause problems in some instances.\n\nIntel MKL libraries are available as part of the \"intel\" modules on Comet. Once this module is loaded, the environment variable MKL_ROOT points to the location of the mkl libraries. The MKL link advisor can be used to ascertain the link line (change the MKL_ROOT aspect appropriately).\n\nIn the example below, we are working with the HPC examples that can be found in\n```\n[user@comet-14-01:~/comet-examples/comet101/MKL] pwd\n/home/user/comet-examples/comet101/MKL\n[user@comet-14-01:~/comet-examples/comet101/MKL] ls -al\ntotal 25991\ndrwxr-xr-x 2 user use300 9 Nov 25 17:20 .\ndrwxr-xr-x 16 user use300 16 Aug 5 19:02 ..\n-rw-r--r-- 1 user use300 325 Aug 5 19:02 compile.txt\n-rw-r--r-- 1 user use300 6380 Aug 5 19:02 pdpttr.c\n-rwxr-xr-x 1 user use300 44825440 Nov 25 16:55 pdpttr.exe\n-rw-r--r-- 1 user use300 188 Nov 25 16:57 scalapack.20294236.comet-07-27.out\n-rw-r--r-- 1 user use300 376 Aug 5 19:02 scalapack.sb\n```\n\nThe file `compile.txt` contains the full command to compile the `pdpttr.c` program statically linking 64 bit scalapack libraries on Comet:\n```\n[user@comet-14-01:~/comet-examples/comet101/MKL] cat compile.txt\nmpicc -o pdpttr.exe pdpttr.c /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64/libmkl_scalapack_lp64.a -Wl,--start-group /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64/libmkl_intel_lp64.a /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64/libmkl_sequential.a /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64/libmkl_core.a /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64/libmkl_blacs_intelmpi_lp64.a -Wl,--end-group -lpthread -lm -ldl\n```\n\nRun the command:\n```\n[user@comet-14-01:~/comet-examples/comet101/MKL] mpicc -o pdpttr.exe pdpttr.c -I$MKL_ROOT/include ${MKL_ROOT}/lib/intel64/libmkl_scalapack_lp64.a -Wl,--start-group ${MKL_ROOT}/lib/intel64/libmkl_intel_lp64.a ${MKL_ROOT}/lib/intel64/libmkl_core.a ${MKL_ROOT}/lib/intel64/libmkl_sequential.a -Wl,--end-group ${MKL_ROOT}/lib/intel64/libmkl_blacs_intelmpi_lp64.a -lpthread -lm\n```\nFor more information on the Intel compilers run: [ifort | icc | icpc] -help\n\n[Back to Top](#top)\n<hr>\n\n### <a name=\"compilers-pgi\"></a>Using the PGI Compilers\nThe PGI compilers can be loaded by executing the following commands at the Linux prompt or placing in your startup file (~/.cshrc or ~/.bashrc)\n\n```\nmodule purge\nmodule load pgi mvapich2_ib\n```\n\nFor AVX support, compile with -fast\n\nFor more information on the PGI compilers: man [pgf90 | pgcc | pgCC]\n\n| |Serial | MPI | OpenMP | MPI+OpenMP|\n|---|---|---|---|---|\n|pgf90 | mpif90 | pgf90 -mp | mpif90 -mp|\n|C | pgcc | mpicc | pgcc -mp | mpicc -mp|\n|C++ | pgCC | mpicxx | pgCC -mp | mpicxx -mp|\n\n[Back to Top](#top)\n<hr>\n\n### <a name=\"compilers-gnu\"></a>Using the GNU Compilers\nThe GNU compilers can be loaded by executing the following commands at the Linux prompt or placing in your startup files (~/.cshrc or ~/.bashrc)\n```\nmodule purge\nmodule load gnu openmpi_ib\n```\n\nFor AVX support, compile with -mavx. Note that AVX support is only available in version 4.7 or later, so it is necessary to explicitly load the gnu/4.9.2 module until such time that it becomes the default.\n\nFor more information on the GNU compilers: man [gfortran | gcc | g++]\n\n| |Serial | MPI | OpenMP | MPI+OpenMP|\n|---|---|---|---|---|\n|Fortran | gfortran | mpif90 | gfortran -fopenmp | mpif90 -fopenmp|\n|C | gcc | mpicc | gcc -fopenmp | mpicc -fopenmp|\n|C++ | g++ | mpicxx | g++ -fopenmp | mpicxx -fopenmp|\n\n\n[Back to Top](#top)\n<hr>\n\n\n## Running Jobs on Comet <a name=\"running-jobs\"></a>\nComet manages computational work via the Simple Linux Utility for Resource Management (SLURM) batch environment. Comet places limits on the number of jobs queued and running on a per group (allocation) and partition basis. Submitting a large number of jobs (especially very short ones) can impact the overall scheduler response for all users. If you are anticipating submitting a lot of jobs, contact the SDSC consulting staff before you submit them. We can work to check if there are bundling options that make your workflow more efficient and reduce the impact on the scheduler\n\nFor more details, see the section on Running job in the Comet User Guide:\nhttp://www.sdsc.edu/support/user_guides/comet.html#running\n\n\n### The Simple Linux Utility for Resource Management (SLURM) <a name=\"running-jobs-slurm\"></a>\n\n<img src=\"images/slurm.png\" alt=\"Simple Linux Utility for Resource Management\" width=\"500px\" />\n\n* “Glue” for parallel computer to schedule and execute jobs\n* Role: Allocate resources within a cluster\n * Nodes (unique IP address)\n * Interconnect/switches\n * Generic resources (e.g. GPUs)\n * Launch and otherwise manage jobs\n\n* Functionality:\n * Prioritize queue(s) of jobs;\n * decide when and where to start jobs;\n * terminate job when done;\n * Appropriate resources;\n * Manage accounts for jobs\n\n* All jobs must be run via the Slurm scheduling infrastructure. There are two types of jobs:\n * [Interactive Jobs](#running-jobs-slurm-interactive)\n * [Batch Jobs](#running-jobs-slurm-batch-submit)\n\n[Back to Top](#top)\n<hr>\n\n### Interactive Jobs: <a name=\"running-jobs-slurm-interactive\">\nInteractive HPC systems allow *real-time* user inputs in order to facilitate code development, real-time data exploration, and visualizations. An interactive job (also referred as interactive session) will provide you with a shell on a compute node in which you can launch your jobs. On Comet, use the ```srun``` command:\n```\nsrun --pty --nodes=1 --ntasks-per-node=24 -p debug -t 00:30:00 --wait 0 /bin/bash\n```\n\nFor more information, see the interactive computing tutorial [here](https://github.com/sdsc/sdsc-summer-institute-2020/blob/master/0_preparation/interactive_computing/README.md).\n\n### Batch Jobs using SLURM: <a name=\"running-jobs-slurm-batch\"></a>\nWhen you run in the batch mode, you submit jobs to be run on the compute nodes using the ```sbatch``` command (described below).\n\nBatch scripts are submitted from the login nodes. You can set environment variables in the shell or in the batch script, including:\n* Partition (also called the qeueing system)\n* Time limit for a job (maximum of 48 hours; longer on request)\n* Number of nodes, tasks per node\n* Memory requirements (if any)\n* Job name, output file location\n* Email info, configuration\n\nBelow is an example of a basic batch script, which shows key features including\n naming the job/output file, selecting the SLURM queue partition, defining the\n number of nodes and ocres, and the length of time that the job will need:\n\n```\n[mthomas@comet-ln3 IBRUN]$ cat hellompi-slurm.sb\n#!/bin/bash\n#SBATCH --job-name=\"hellompi\"\n#SBATCH --output=\"hellompi.%j.%N.out\"\n#SBATCH --partition=compute\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=24\n#SBATCH --export=ALL\n#SBATCH -t 00:30:00\n\n#Define user environment\nsource /etc/profile.d/modules.sh\nmodule purge\nmodule load intel\nmodule load mvapich2_ib\n\n#This job runs with 2 nodes, 24 cores per node for a total of 48 cores.\n#ibrun in verbose mode will give binding detail\n\nibrun -v ../hello_mpi\n```\n\nNote that we have included configuring the user environment by purging and\nthen loading the necessary modules. While not required, it is a good habit\nto develop when building batch scripts.\n\n[Back to Top](#top)\n<hr>\n\n### Slurm Partitions <a name=\"slurm-partitions\"></a>\nComet places limits on the number of jobs queued and running on a per group (allocation) and partition basis. Please note that submitting a large number of jobs (especially very short ones) can impact the overall scheduler response for all users.\n\n<img src=\"images/comet-queue-names.png\" alt=\"Comet Queue Names\" width=\"500px\" />\n\nSpecified using -p option in batch script. For example:\n```\n#SBATCH -p gpu\n```\n[Back to Top](#top)\n<hr>\n\n### Slurm Commands: <a name=\"slurm-commands\"></a>\nHere are a few key Slurm commands. For more information, run the `man slurm` or see this page:\n\n* To Submit jobs using the `sbatch` command:\n\n```\n$ sbatch Localscratch-slurm.sb \nSubmitted batch job 8718049\n```\n* To check job status using the squeue command:\n```\n$ squeue -u $USER\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           8718049   compute localscr mahidhar PD       0:00      1 (Priority)\n```\n* Once the job is running, you will see the job status change:\n```\n$ squeue -u $USER\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           8718064     debug localscr mahidhar  R       0:02      1 comet-14-01\n```\n* To cancel a job, use the `scancel` along with the `JOBID`:\n * $scancel <jobid>\n\n### Command Line Jobs <a name=\"running-jobs-cmdline\"></a>\n The login nodes are meant for compilation, file editing, simple data analysis, and other tasks that use minimal compute resources. <em>Do not run parallel or large jobs on the login nodes - even for simple tests</em>. Even if you could run a simple test on the command line on the login node, full tests should not be run on the login node because the performance will be adversely impacted by all the other tasks and login activities of the other users who are logged onto the same node. For example, at the moment that this note was written, a `gzip` process was consuming 98% of the CPU time:\n ```\n [mthomas@comet-ln3 OPENMP]$ top\n ...\n PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND \n 19937 XXXXX 20 0 4304 680 300 R 98.2 0.0 0:19.45 gzip\n ```\n\nCommands that you type into the terminal and run on the sytem are considered *jobs* and they consume resources. <em>Computationally intensive jobs should be run only on the compute nodes and not the login nodes</em>.\n\n[Back to Top](#top)\n<hr>\n\n## <a name=\"hands-on\"></a>Hands-on Examples\n* [Compiling and Running GPU/CUDA Jobs](#comp-and-run-cuda-jobs)\n * [GPU Hello World (GPU) ](#hello-world-gpu)\n * [GPU Enumeration ](#enum-gpu)\n * [CUDA Mat-Mult](#mat-mul-gpu)\n* [Compiling and Running CPU Jobs](#comp-and-run-cpu-jobs)\n * [Hello World (MPI)](#hello-world-mpi)\n * [Hello World (OpenMPI)](#hello-world-omp)\n * [Compiling and Running Hybrid (MPI + OpenMP) Jobs](#hybrid-mpi-omp)\n\n## <a name=\"comp-and-run-cuda-jobs\"></a>Compiling and Running GPU/CUDA Jobs\n<b>Sections:</b>\n* [GPU Hello World (GPU) ](#hello-world-gpu)\n* [GPU Enumeration ](#enum-gpu)\n* [CUDA Mat-Mult](#mat-mul-gpu)\n\nNote: Comet provides both NVIDIA K80 and P100 GPU-based resources. These GPU nodes\nare allocated as separate resources. Make sure you have enough allocations and that\nyou are using the right account. For more details and current information about the\nComet GPU nodes, see the [Comet User Guide](https://www.sdsc.edu/support/user_guides/comet.html#gpu).\n\n<b> Comet GPU Hardware: </b> <br>\n<a name=\"gpu-hardware\"></a><img src=\"images/comet-gpu-hardware.png\" alt=\"Comet GPU Hardware\" width=\"500px\" />\n\n## In order to compile the CUDA code, you need to load the CUDA module and verify\nthat you have access to the CUDA compile command, `nvcc:`\n```\n[mthomas@comet-ln3:~/comet101] module list\nCurrently Loaded Modulefiles:\n 1) intel/2018.1.163 2) mvapich2_ib/2.3.2\n[mthomas@comet-ln3:~/comet101] module purge\n[mthomas@comet-ln3:~/comet101] module load cuda\n[mthomas@comet-ln3:~/comet101] module list\nCurrently Loaded Modulefiles:\n 1) cuda/10.1\n [mthomas@comet-ln3:~/comet101] which nvcc\n /usr/local/cuda-10.1/bin/nvcc\n```\n\n[Back to GPU/CUDA Jobs](#comp-and-run-cuda-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n### <a name=\"hello-world-gpu\"></a>GPU/CUDA Example: Hello World\n<b>Subsections:</b>\n* [GPU Hello World: Compiling](#hello-world-gpu-compile)\n* [GPU Hello World: Batch Script Submission](#hello-world-gpu-batch-submit)\n* [GPU Hello World: Batch Job Output](#hello-world-gpu-batch-output)\n\n#### <a name=\"hello-world-gpu-compile\"></a>GPU Hello World: Compiling\nSimple hello runs a cuda command to get the device count\non the node that job is assigned to. :\n```\n[mthomas@comet-ln3:~/comet101] cd CUDA/hello_cuda\n[mthomas@comet-ln3:~/comet101/CUDA/hello_cuda] ll\ntotal 30\ndrwxr-xr-x 2 mthomas use300 4 Apr 16 01:59 .\ndrwxr-xr-x 4 mthomas use300 11 Apr 16 01:57 ..\n-rw-r--r-- 1 mthomas use300 313 Apr 16 01:59 hello_cuda.cu\n-rw-r--r-- 1 mthomas use300 269 Apr 16 01:58 hello_cuda.sb\n[mthomas@comet-ln3:~/comet101/CUDA/cuda_hello] cat hello_cuda.cu\n/*\n * hello_cuda.cu\n * Copyright 1993-2010 NVIDIA Corporation.\n * All right reserved\n */\n #include <stdio.h>\n #include <stdlib.h>\n int main( void )\n {\n int deviceCount;\n cudaGetDeviceCount( &deviceCount );\n printf(\"Hello, Webinar Participants! You have %d devices\\n\", deviceCount );\n return 0;\n }\n```\n* Compile using the `nvcc`</b> command:\n```\n[mthomas@comet-ln3:~/comet101/CUDA/cuda_hello] nvcc -o hello_cuda hello_cuda.cu\n[mthomas@comet-ln3:~/comet101/CUDA/cuda_hello] ll hello_cuda\n-rwxr-xr-x 1 user use300 517437 Apr 10 19:35 hello_cuda\n-rw-r--r-- 1 user use300 304 Apr 10 19:35 hello_cuda.cu\n[comet-ln2:~/cuda/hello_cuda]\n\n```\n[Back to GPU/CUDA Jobs](#comp-and-run-cuda-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n#### <a name=\"hello-world-gpu-batch-submit\"></a>GPU Hello World: Batch Script Submit\n\n* GPU jobs can be run via the slurm scheduler, or on interactive nodes.\n* The slurm scheduler batch script is shown below:\n```\n[mthomas@comet-ln3:~/comet101/CUDA/cuda_hello] cat hello_cuda.sb\n#!/bin/bash\n#SBATCH --job-name=\"hello_cuda\"\n#SBATCH --output=\"hello_cuda.%j.%N.out\"\n#SBATCH --partition=gpu-shared\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=12\n#SBATCH --gres=gpu:2\n#SBATCH -t 01:00:00\n\n# Define the user environment\nsource /etc/profile.d/modules.sh\nmodule purge\nmodule load intel\nmodule load mvapich2_ib\n#Load the cuda module\nmodule load cuda\n\n#Run the job\n./hello_cuda\n```\n* Some of the batch script variables are described below. For more details see\nthe Comet user guide.\n* GPU nodes can be accessed via either the \"gpu\" or the \"gpu-shared\" partitions:\n```\n#SBATCH -p gpu \n```\nor\n```\n#SBATCH -p gpu-shared\n```\n\nIn addition to the partition name (required), the type of gpu (optional) and \nthe individual GPUs are scheduled as a resource.\n```\n#SBATCH --gres=gpu[:type]:n\n```\n\nGPUs will be allocated on a first available, first schedule basis, unless specified with the [type] option, where type can be <b>`k80`</b> or <b>`p100`</b> Note: type is case sensitive.\n```\n#SBATCH --gres=gpu:4 #first available gpu node\n#SBATCH --gres=gpu:k80:4 #only k80 nodes\n#SBATCH --gres=gpu:p100:4 #only p100 nodes\n```\n\n<b>Submit the job</b> <br>\n\nTo run the job, type the batch script submission command:\n```\n[mthomas@comet-ln3:~/comet101/CUDA/cuda_hello] sbatch hello_cuda.sb\nSubmitted batch job 32663172\n[mthomas@comet-ln3:~/comet101/CUDA/cuda_hello]\n\n```\n\n<b>Monitor the job until it is finished:</b>\n```\n[user@comet-ln2:~/cuda/hello_cuda] squeue -u mthomas\n[mthomas@comet-ln3:~/comet101/CUDA/cuda_hello] sbatch hello_cuda.sb\nSubmitted batch job 32663081\n[mthomas@comet-ln3:~/comet101/CUDA/cuda_hello] squeue -u mthomas\n JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n 32663081 gpu-share hello_cu mthomas PD 0:00 1 (Resources)\n```\n\n[Back to GPU/CUDA Jobs](#comp-and-run-cuda-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n#### <a name=\"hello-world-gpu-batch-output\"></a>GPU Hello World: Batch Job Output\n```\n[mthomas@comet-ln3:~/comet101/CUDA/cuda_hello] cat hello_cuda.32663172.comet-30-04.out\n\nHello, Webinar Participants! You have 2 devices\n\n[mthomas@comet-ln3:~/comet101/CUDA/cuda_hello]\n```\n\n[Back to GPU/CUDA Jobs](#comp-and-run-cuda-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n\n### <a name=\"enum-gpu\"></a>GPU/CUDA Example: Enumeration\n\nSections:\n* [GPU Enumeration: Compiling](#enum-gpu-compile)\n* [GPU Enumeration: Batch Script Submission](#enum-gpu-batch-submit)\n* [GPU Enumeration: Batch Job Output](#enum-gpu-batch-output )\n\n<hr>\n\n#### <a name=\"enum-gpu-compile\"></a>GPU Enumeration: Compiling\n\n<b>GPU Enumeration Code:</b>\nThis code accesses the cudaDeviceProp object and returns information about the devices on the node. The list below is only some of the information that you can look for. The property values can be used to dynamically allocate or distribute your compute threads accross the GPU hardware in response to the GPU type.\n```\n[user@comet-ln2:~/cuda/gpu_enum] cat gpu_enum.cu\n#include <stdio.h>\n\nint main( void ) {\n cudaDeviceProp prop;\n int count;\n printf( \" --- Obtaining General Information for CUDA devices ---\\n\" );\n cudaGetDeviceCount( &count ) ;\n for (int i=0; i< count; i++) {\n cudaGetDeviceProperties( &prop, i ) ;\n printf( \" --- General Information for device %d ---\\n\", i );\n printf( \"Name: %s\\n\", prop.name );\n\n printf( \"Compute capability: %d.%d\\n\", prop.major, prop.minor );\n printf( \"Clock rate: %d\\n\", prop.clockRate );\n printf( \"Device copy overlap: \" );\n\n if (prop.deviceOverlap)\n printf( \"Enabled\\n\" );\n else\n printf( \"Disabled\\n\");\n\n printf( \"Kernel execution timeout : \" );\n\n if (prop.kernelExecTimeoutEnabled)\n printf( \"Enabled\\n\" );\n else\n printf( \"Disabled\\n\" );\n\n printf( \" --- Memory Information for device %d ---\\n\", i );\n printf( \"Total global mem: %ld\\n\", prop.totalGlobalMem );\n printf( \"Total constant Mem: %ld\\n\", prop.totalConstMem );\n printf( \"Max mem pitch: %ld\\n\", prop.memPitch );\n printf( \"Texture Alignment: %ld\\n\", prop.textureAlignment );\n printf( \" --- MP Information for device %d ---\\n\", i );\n printf( \"Multiprocessor count: %d\\n\", prop.multiProcessorCount );\n printf( \"Shared mem per mp: %ld\\n\", prop.sharedMemPerBlock );\n printf( \"Registers per mp: %d\\n\", prop.regsPerBlock );\n printf( \"Threads in warp: %d\\n\", prop.warpSize );\n printf( \"Max threads per block: %d\\n\", prop.maxThreadsPerBlock );\n printf( \"Max thread dimensions: (%d, %d, %d)\\n\", prop.maxThreadsDim[0], prop.maxThreadsDim[1], prop.maxThreadsDim[2] );\n printf( \"Max grid dimensions: (%d, %d, %d)\\n\", prop.maxGridSize[0], prop.maxGridSize[1], prop.maxGridSize[2] );\n printf( \"\\n\" );\n }\n}\n```\n\nTo compile: check your environment and use the CUDA <b>`nvcc`</b> command:\n```\n[comet-ln2:~/cuda/gpu_enum] module purge\n[comet-ln2:~/cuda/gpu_enum] which nvcc\n/usr/bin/which: no nvcc in (/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/sdsc/bin:/opt/sdsc/sbin:/opt/ibutils/bin:/usr/java/latest/bin:/opt/pdsh/bin:/opt/rocks/bin:/opt/rocks/sbin:/home/user/bin)\n[comet-ln2:~/cuda/gpu_enum] module load cuda\n[comet-ln2:~/cuda/gpu_enum] which nvcc\n/usr/local/cuda-7.0/bin/nvcc\n[comet-ln2:~/cuda/gpu_enum] nvcc -o gpu_enum -I. gpu_enum.cu\n[comet-ln2:~/cuda/gpu_enum] ll gpu_enum\n-rwxr-xr-x 1 user use300 517632 Apr 10 18:39 gpu_enum\n[comet-ln2:~/cuda/gpu_enum]\n```\n[Back to GPU/CUDA Jobs](#comp-and-run-cuda-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n#### <a name=\"enum-gpu-batch-submit\"></a>GPU Enumeration: Batch Script Submission\n<b>Contents of the Slurm script </b>\nScript is asking for 1 GPU.\n\n```\n[comet-ln2: ~/cuda/gpu_enum] cat gpu_enum.sb\n#!/bin/bash\n#SBATCH --job-name=\"gpu_enum\"\n#SBATCH --output=\"gpu_enum.%j.%N.out\"\n#SBATCH --partition=gpu-shared # define GPU partition\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=6\n#SBATCH --gres=gpu:1 # define type of GPU\n#SBATCH -t 00:05:00\n\n# Define the user environment\nsource /etc/profile.d/modules.sh\nmodule purge\nmodule load intel\nmodule load mvapich2_ib\n#Load the cuda module\nmodule load cuda\n\n#Run the job\n./gpu_enum\n\n```\n\n<b>Submit the job </b>\n* To run the job, type the batch script submission command:\n```\n[mthomas@comet-ln3:~/comet101/CUDA/gpu_enum] sbatch hello_cuda.sb\nSubmitted batch job 32663364\n\n```\n<b>Monitor the job </b>\n* You can monitor the job until it is finished using the `sqeue` command:\n```\n[mthomas@comet-ln3:~/comet101/CUDA/gpu_enum] squeue -u mthomas\n JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n 32663364 gpu-share gpu_enum mthomas PD 0:00 1 (Resources)\n\n```\n[Back to GPU/CUDA Jobs](#comp-and-run-cuda-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n#### <a name=\"enum-gpu-batch-output\"></a>GPU Enumeration: Batch Job Output\n\n* Output from script is for multiple devices, which is what was specified in script.\n\n```\n[user@comet-ln2:~/cuda/gpu_enum] cat gpu_enum.22527745.comet-31-10.out\n --- Obtaining General Information for CUDA devices ---\n --- General Information for device 0 ---\n --- Obtaining General Information for CUDA devices ---\n --- General Information for device 0 ---\nName: Tesla P100-PCIE-16GB\nCompute capability: 6.0\nClock rate: 1328500\nDevice copy overlap: Enabled\nKernel execution timeout : Disabled\n --- Memory Information for device 0 ---\nTotal global mem: 17071734784\nTotal constant Mem: 65536\nMax mem pitch: 2147483647\nTexture Alignment: 512\n --- MP Information for device 0 ---\nMultiprocessor count: 56\nShared mem per mp: 49152\nRegisters per mp: 65536\nThreads in warp: 32\nMax threads per block: 1024\nMax thread dimensions: (1024, 1024, 64)\nMax grid dimensions: (2147483647, 65535, 65535)\n```\n* If we change the batch script to ask for 2 devices (see line 8):\n```\n 1 #!/bin/bash\n 2 #SBATCH --job-name=\"gpu_enum\"\n 3 #SBATCH --output=\"gpu_enum.%j.%N.out\"\n 4 #SBATCH --partition=gpu-shared # define GPU partition\n 5 #SBATCH --nodes=1\n 6 #SBATCH --ntasks-per-node=6\n 7 ####SBATCH --gres=gpu:1 # define type of GPU\n 8 #SBATCH --gres=gpu:2 # first available\n 9 #SBATCH -t 00:05:00\n 10\n 11 # Define the user environment\n 12 source /etc/profile.d/modules.sh\n 13 module purge\n 14 module load intel\n 15 module load mvapich2_ib\n 16 #Load the cuda module\n 17 module load cuda\n 18\n 19 #Run the job\n 20 ./gpu_enum\n```\n\nThe output will show information for two devices:\n```\n[mthomas@comet-ln3:~/comet101/CUDA/gpu_enum] sbatch gpu_enum.sb\n!Submitted batch job 32663404\n[mthomas@comet-ln3:~/comet101/CUDA/gpu_enum] squeue -u mthomas\n JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n 32663404 gpu-share gpu_enum mthomas CG 0:02 1 comet-33-03\n [mthomas@comet-ln3:~/comet101/CUDA/gpu_enum] cat gpu_enumX.32663404.comet-33-03.out\n --- Obtaining General Information for CUDA devices ---\n --- General Information for device 0 ---\n Name: Tesla P100-PCIE-16GB\n Compute capability: 6.0\n Clock rate: 1328500\n Device copy overlap: Enabled\n Kernel execution timeout : Disabled\n --- Memory Information for device 0 ---\n Total global mem: 17071734784\n Total constant Mem: 65536\n Max mem pitch: 2147483647\n Texture Alignment: 512\n --- MP Information for device 0 ---\n Multiprocessor count: 56\n Shared mem per mp: 49152\n Registers per mp: 65536\n Threads in warp: 32\n Max threads per block: 1024\n Max thread dimensions: (1024, 1024, 64)\n Max grid dimensions: (2147483647, 65535, 65535)\n\n --- General Information for device 1 ---\n Name: Tesla P100-PCIE-16GB\n Compute capability: 6.0\n Clock rate: 1328500\n Device copy overlap: Enabled\n Kernel execution timeout : Disabled\n --- Memory Information for device 1 ---\n Total global mem: 17071734784\n Total constant Mem: 65536\n Max mem pitch: 2147483647\n Texture Alignment: 512\n --- MP Information for device 1 ---\n Multiprocessor count: 56\n Shared mem per mp: 49152\n Registers per mp: 65536\n Threads in warp: 32\n Max threads per block: 1024\n Max thread dimensions: (1024, 1024, 64)\n Max grid dimensions: (2147483647, 65535, 65535)\n```\n\n[Back to GPU/CUDA Jobs](#comp-and-run-cuda-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n### <a name=\"mat-mul-gpu\"></a>GPU/CUDA Example: Matrix-Multiplication\n<b>Subsections:</b>\n* [Matrix Mult. (GPU): Compiling](#mat-mul-gpu-compile)\n* [Matrix Mult. (GPU): Batch Script Submission](#mat-mul-gpu-batch-submit)\n* [Matrix Mult. (GPU): Batch Job Output](#mat-mul-gpu-batch-output )\n\n#### <a name=\"mat-mul-gpu\"></a>CUDA Example: Matrix-Multiplication\n<b>Change to the CUDA Matrix-Multiplication example directory:</b>\n```\n[mthomas@comet-ln3:~/comet101/CUDA/matmul] ll\ntotal 454\ndrwxr-xr-x 2 mthomas use300 11 Apr 16 02:59 .\ndrwxr-xr-x 5 mthomas use300 5 Apr 16 02:37 ..\n-rw-r--r-- 1 mthomas use300 253 Apr 16 01:56 cuda_matmul.sb\n-rw-r--r-- 1 mthomas use300 5106 Apr 16 01:46 exception.h\n-rw-r--r-- 1 mthomas use300 1168 Apr 16 01:46 helper_functions.h\n-rw-r--r-- 1 mthomas use300 29011 Apr 16 01:46 helper_image.h\n-rw-r--r-- 1 mthomas use300 23960 Apr 16 01:46 helper_string.h\n-rw-r--r-- 1 mthomas use300 15414 Apr 16 01:46 helper_timer.h\n-rwxr-xr-x 1 mthomas use300 652768 Apr 16 01:46 matmul\n-rw-r--r-- 1 mthomas use300 13482 Apr 16 02:36 matmul.cu\n-rw-r--r-- 1 mthomas use300 370 Apr 16 02:59 matmul.sb\n```\n\n[Back to GPU/CUDA Jobs](#comp-and-run-cuda-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n#### <a name=\"mat-mul-gpu-compile\"></a>Compiling CUDA Example (GPU)\n\n<b> Compile the code:</b>\n```\n[user@comet-ln2 CUDA]$ nvcc -o matmul -I. matrixMul.cu\n[user@comet-ln2 CUDA]$ ll\ntotal 172\ndrwxr-xr-x 2 user user300 13 Aug 6 00:53 .\ndrwxr-xr-x 16 user user300 16 Aug 5 19:02 ..\n-rw-r--r-- 1 user user300 458 Aug 6 00:35 CUDA.18347152.comet-33-02.out\n-rw-r--r-- 1 user user300 458 Aug 6 00:37 CUDA.18347157.comet-33-02.out\n-rw-r--r-- 1 user user300 446 Aug 5 19:02 CUDA.8718375.comet-30-08.out\n-rw-r--r-- 1 user user300 253 Aug 5 19:02 cuda.sb\n-rw-r--r-- 1 user user300 5106 Aug 5 19:02 exception.h\n-rw-r--r-- 1 user user300 1168 Aug 5 19:02 helper_functions.h\n-rw-r--r-- 1 user user300 29011 Aug 5 19:02 helper_image.h\n-rw-r--r-- 1 user user300 23960 Aug 5 19:02 helper_string.h\n-rw-r--r-- 1 user user300 15414 Aug 5 19:02 helper_timer.h\n-rwxr-xr-x 1 user user300 533168 Aug 6 00:53 matmul\n-rw-r--r-- 1 user user300 13482 Aug 6 00:50 matrixMul.cu\n ```\n\n[Back to GPU/CUDA Jobs](#comp-and-run-cuda-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n#### <a name=\"mat-mul-gpu-batch-submit\"></a>Matrix Mult. (GPU): Batch Script Submission\n\n<b>Contents of the slurm script:</b>\n```\n[user@comet-ln2 CUDA]$ cat cuda.sb\n#!/bin/bash\n#SBATCH --job-name=\"matmul\"\n#SBATCH --output=\"matmul.%j.%N.out\"\n#SBATCH --partition=gpu-shared\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=6\n#SBATCH --gres=gpu:1\n#SBATCH -t 00:10:00\n\n# Define the user environment\nsource /etc/profile.d/modules.sh\nmodule purge\nmodule load intel\nmodule load mvapich2_ib\n#Load the cuda module\nmodule load cuda\n\n#Run the job\n./matmul\n```\n<b> Submit the job:</b>\n```\n[mthomas@comet-ln3:~/comet101/CUDA/matmul] sbatch matmul.sb\nSubmitted batch job 32663647\n```\n<b>Monitor the job:</b>\n```\n[mthomas@comet-ln3:~/comet101/CUDA/matmul] squeue -u mthomas\n JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n 32663647 gpu-share matmul mthomas PD 0:00 1 (Resources)\n[mthomas@comet-ln3:~/comet101/CUDA/matmul]\n\n```\n[Back to GPU/CUDA Jobs](#comp-and-run-cuda-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n#### <a name=\"mat-mul-gpu-batch-output\"></a>Matrix Mult. (GPU): Batch Job Output\n\n```\n[mthomas@comet-ln3:~/comet101/CUDA/matmul] cat matmul.32663647.comet-33-03.out\n[Matrix Multiply Using CUDA] - Starting...\nGPU Device 0: \"Tesla P100-PCIE-16GB\" with compute capability 6.0\n\nMatrixA(320,320), MatrixB(640,320)\nComputing result using CUDA Kernel...\ndone\nPerformance= 1676.99 GFlop/s, Time= 0.078 msec, Size= 131072000 Ops, WorkgroupSize= 1024 threads/block\nChecking computed result for correctness: Result = PASS\n\nNOTE: The CUDA Samples are not meant for performance measurements. Results may\nvary when GPU Boost is enabled.\n```\n\n[Back to GPU/CUDA Jobs](#comp-and-run-cuda-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n\n## Compiling and Running CPU Jobs: <a name=\"comp-and-run-cpu-jobs\"></a>\n<b>Sections:</b>\n* [Hello World (MPI)](#hello-world-mpi)\n* [Hello World (OpenMP)](#hello-world-omp)\n* [Running Hybrid (MPI + OpenMP) Jobs](#hybrid-mpi-omp)\n\n\n### <a name=\"hello-world-mpi\"></a>Hello World (MPI)\n<b>Subsections:</b>\n* [Hello World (MPI): Source Code](#hello-world-mpi-source)\n* [Hello World (MPI): Compiling](#hello-world-mpi-compile)\n* [Hello World (MPI): Interactive Jobs](#hello-world-mpi-interactive)\n* [Hello World (MPI): Batch Script Submission](#hello-world-mpi-batch-submit)\n* [Hello World (MPI): Batch Script Output](#hello-world-mpi-batch-output)\n\n\n#### CPU Hello World: Source code: <#hello-world-mpi-source>\nChange to the MPI examples directory (assuming you already copied the ):\n```\n[mthomas@comet-ln3 comet101]$ cd MPI\n[mthomas@comet-ln3 MPI]$ ll\n[mthomas@comet-ln3:~/comet101/MPI] ll\ntotal 498\ndrwxr-xr-x 4 mthomas use300 7 Apr 16 01:11 .\ndrwxr-xr-x 6 mthomas use300 6 Apr 15 20:10 ..\n-rw-r--r-- 1 mthomas use300 336 Apr 15 15:47 hello_mpi.f90\ndrwxr-xr-x 2 mthomas use300 3 Apr 16 01:02 IBRUN\ndrwxr-xr-x 2 mthomas use300 3 Apr 16 00:57 MPIRUN_RSH\n``\n[mthomas@comet-ln3 OPENMP]$cat hello_mpi.f90\n! Fortran example \n program hello\n include 'mpif.h'\n integer rank, size, ierror, tag, status(MPI_STATUS_SIZE)\n\n call MPI_INIT(ierror)\n call MPI_COMM_SIZE(MPI_COMM_WORLD, size, ierror)\n call MPI_COMM_RANK(MPI_COMM_WORLD, rank, ierror)\n print*, 'node', rank, ': Hello and Welcome to Webinar Participants!'\n call MPI_FINALIZE(ierror)\n end\n```\n\nCompile the code:\n```\n[mthomas@comet-ln3 MPI]$ mpif90 -o hello_mpi hello_mpi.f90\n[mthomas@comet-ln3:~/comet101/MPI] ll\ntotal 498\ndrwxr-xr-x 4 mthomas use300 7 Apr 16 01:11 .\ndrwxr-xr-x 6 mthomas use300 6 Apr 15 20:10 ..\n-rw-r--r-- 1 mthomas use300 77 Apr 16 01:08 compile.txt\n-rwxr-xr-x 1 mthomas use300 750288 Apr 16 01:11 hello_mpi\n-rw-r--r-- 1 mthomas use300 336 Apr 15 15:47 hello_mpi.f90\ndrwxr-xr-x 2 mthomas use300 3 Apr 16 01:02 IBRUN\ndrwxr-xr-x 2 mthomas use300 3 Apr 16 00:57 MPIRUN_RSH\n```\nNote: The two directories that contain batch scripts needed to run the jobs using the parallel/slurm environment.\n\n* First, we should verify that the user environment is correct for running the examples we will work with in this tutorial.\n```\n[mthomas@comet-ln3 MPI]$ module list\nCurrently Loaded Modulefiles:\n1) intel/2018.1.163 2) mvapich2_ib/2.3.2\n```\n* If you have trouble with your modules, you can remove the existing environment (purge) and then reload them. After purging, the PATH variable has fewer path directories available:\n```\n[mthomas@comet-ln3:~] module purge\n[mthomas@comet-ln3:~] echo $PATH\n/home/mthomas/miniconda3/bin:/home/mthomas/miniconda3/condabin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/sdsc/bin:/opt/sdsc/sbin:/opt/ibutils/bin:/opt/pdsh/bin:/opt/rocks/bin:/opt/rocks/sbin:/home/mthomas/bin\n```\n* Next, you reload the modules that you need:\n```\n[mthomas@comet-ln3 ~]$ module load intel\n[mthomas@comet-ln3 ~]$ module load mvapich2_ib\n```\n* You will see that there are more binaries in the PATH:\n```\n[mthomas@comet-ln3:~] echo $PATH\n/opt/mvapich2/intel/ib/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64:/home/mthomas/miniconda3/bin:/home/mthomas/miniconda3/condabin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/sdsc/bin:/opt/sdsc/sbin:/opt/ibutils/bin:/opt/pdsh/bin:/opt/rocks/bin:/opt/rocks/sbin:/home/mthomas/bin\n```\n\n[Back to CPU Jobs](#comp-and-run-cpu-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n#### Hello World (MPI): Compiling: <a name=\"hello-world-mpi-compile\"></a>\n\n* Compile the MPI hello world code.\n* For this, we use the command `mpif90`, which is loaded into your environment when you loaded the intel module above.\n* To see where the command is located, use the `which` command:\n```\n[mthomas@comet-ln3 MPI]$ which mpif90\n/opt/mvapich2/intel/ib/bin/mpif90\n```\n* Compile the code:\n```\nmpif90 -o hello_mpi hello_mpi.f90\n```\n\n* Verify that the executable has been created:\n\n```\n[mthomas@comet-ln3:~/comet101/MPI] ll\ntotal 498\ndrwxr-xr-x 4 mthomas use300 7 Apr 16 01:11 .\ndrwxr-xr-x 6 mthomas use300 6 Apr 15 20:10 ..\n-rwxr-xr-x 1 mthomas use300 750288 Apr 16 01:11 hello_mpi\n-rw-r--r-- 1 mthomas use300 336 Apr 15 15:47 hello_mpi.f90\ndrwxr-xr-x 2 mthomas use300 3 Apr 16 01:02 IBRUN\ndrwxr-xr-x 2 mthomas use300 3 Apr 16 00:57 MPIRUN_RSH\n```\n\n* In the next sections, we will see how to run parallel code using two environments:\n * Running a parallel job on an _Interactive_ compute node\n * Running parallel code using the batch queue system\n\n[Back to CPU Jobs](#comp-and-run-cpu-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n#### Hello World (MPI): Interactive Jobs: <a name=\"hello-world-mpi-interactive\"></a>\n\n* To run MPI (or other executables) from the command line, you need to use the \"Interactive\" nodes.\n* To launch the nodes (to get allocated a set of nodes), use the `srun` command. This example will request one node, all 24 cores, in the debug partition for 30 minutes:\n```\n[mthomas@comet-ln3:~/comet101/MPI] date\nThu Apr 16 01:21:48 PDT 2020\n[mthomas@comet-ln3:~/comet101/MPI] srun --pty --nodes=1 --ntasks-per-node=24 -p debug -t 00:30:00 --wait 0 /bin/bash\n[mthomas@comet-14-01:~/comet101/MPI] date\nThu Apr 16 01:22:42 PDT 2020\n[mthomas@comet-14-01:~/comet101/MPI] hostname\ncomet-14-01.sdsc.edu\n```\n* Note:\n * You will know when you have an interactive node because the srun command\n will return and you will be on a different host.\n * Note: If the cluster is very busy, it may take some time to obtain the nodes. \n* Once you have the interactive session, your MPI code will be allowed to execute on the command line.\n```\n[mthomas@comet-14-01 MPI]$ mpirun -np 4 ./hello_mpi\n node 0 : Hello and Welcome to Webinar Participants!\n node 1 : Hello and Welcome to Webinar Participants!\n node 2 : Hello and Welcome to Webinar Participants!\n node 3 : Hello and Welcome to Webinar Participants!\n[mthomas@comet-14-01 MPI]$\n```\n\nWhen you are done testing code, exit the Interactive session.\n\n[Back to CPU Jobs](#comp-and-run-cpu-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n#### Hello World (MPI): Batch Script Submission: <a name=\"hello-world-mpi-batch-submit\"></a>\nTo submit jobs to the Slurm queuing system, you need to create a slurm batch job script and\nsubmit it to the queuing system.\n\n* Change directories to the IBRUN directory using the `hellompi-slurm.sb` batch script:\n```\n[mthomas@comet-ln3 MPI]$ cd IBRUN/\n[mthomas@comet-ln3 IBRUN]$ cat hellompi-slurm.sb\n#!/bin/bash\n#SBATCH --job-name=\"hellompi\"\n#SBATCH --output=\"hellompi.%j.%N.out\"\n#SBATCH --partition=compute\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=24\n#SBATCH --export=ALL\n#SBATCH -t 00:30:00\n\n# load the user environment\nsource /etc/profile.d/modules.sh\nmodule purge\nmodule load intel\nmodule load mvapich2_ib\n\n#This job runs with 2 nodes, 24 cores per node for a total of 48 cores.\n#ibrun in verbose mode will give binding detail\n\nibrun -v ../hello_mpi\n\n```\n* to run the job, use the command below:\n```\n[mthomas@comet-ln3 IBRUN]$ sbatch hellompi.sb\nSubmitted batch job 32662205\n```\n* In some cases, you may have access to a reservation queue, use the command below:\n``` \nsbatch --res=SI2018DAY1 hellompi-slurm.sb\n```\n\n[Back to CPU Jobs](#comp-and-run-cpu-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n#### Hello World (MPI): Batch Script Output: <a name=\"hello-world-mpi-batch-output\"></a>\n\n* Check job status using the `squeue` command.\n```\n[mthomas@comet-ln3 IBRUN]$ sbatch hellompi-slurm.sb; squeue -u username\nSubmitted batch job 18345138\n JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n 32662205 compute hellompi username PD 0:00 2 (None)\n....\n\n[mthomas@comet-ln3 IBRUN]$ squeue -u username\n JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n 32662205 compute hellompi username R 0:07 2 comet-21-[47,57]\n[mthomas@comet-ln3 IBRUN]$ squeue -u username\n JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n 32662205 compute hellompi username CG 0:08 2 comet-21-[47,57]\n```\n * Note: You will see the `ST` column information change when the job status changes: new jobs go into `SP` (pending); after some time it moves to `R` (running): when completed, the state changes to `CG` (completed)\n * the JOBID is the job identifer and can be used to track or cancel the job. It is also used as part of the output file name.\n\n* Look at the directory for and output file with the job id as part of the name:\n```\n[mthomas@comet-ln3 IBRUN]$\ntotal 48\ndrwxr-xr-x 2 mthomas use300 4 Apr 16 01:31 .\ndrwxr-xr-x 4 mthomas use300 7 Apr 16 01:11 ..\n-rw-r--r-- 1 mthomas use300 2873 Apr 16 01:31 hellompi.32662205.comet-20-03.out\n-rw-r--r-- 1 mthomas use300 341 Apr 16 01:30 hellompi-slurm.sb\n```\n\n* To see the contents of the output file, use the `cat` command:\n```\n[mthomas@comet-ln3 IBRUN]$ cat hellompi.32662205.comet-20-03.out\nIBRUN: Command is ../hello_mpi\nIBRUN: Command is /home/username/comet-examples/comet101/MPI/hello_mpi\nIBRUN: no hostfile mod needed\nIBRUN: Nodefile is /tmp/0p4Nbx12u1\n\nIBRUN: MPI binding policy: compact/core for 1 threads per rank (12 cores per socket)\nIBRUN: Adding MV2_USE_OLD_BCAST=1 to the environment\nIBRUN: Adding MV2_CPU_BINDING_LEVEL=core to the environment\nIBRUN: Adding MV2_ENABLE_AFFINITY=1 to the environment\nIBRUN: Adding MV2_DEFAULT_TIME_OUT=23 to the environment\nIBRUN: Adding MV2_CPU_BINDING_POLICY=bunch to the environment\nIBRUN: Adding MV2_USE_HUGEPAGES=0 to the environment\nIBRUN: Adding MV2_HOMOGENEOUS_CLUSTER=0 to the environment\nIBRUN: Adding MV2_USE_UD_HYBRID=0 to the environment\nIBRUN: Added 8 new environment variables to the execution environment\nIBRUN: Command string is [mpirun_rsh -np 48 -hostfile /tmp/0p4Nbx12u1 -export-all /home/username/comet-examples/comet101/MPI/hello_mpi]\n node 18 : Hello and Welcome to Webinar Participants!\n node 17 : Hello and Welcome to Webinar Participants!\n node 20 : Hello and Welcome to Webinar Participants!\n node 21 : Hello and Welcome to Webinar Participants!\n node 22 : Hello and Welcome to Webinar Participants!\n node 5 : Hello and Welcome to Webinar Participants!\n node 3 : Hello and Welcome to Webinar Participants!\n node 6 : Hello and Welcome to Webinar Participants!\n node 16 : Hello and Welcome to Webinar Participants!\n node 19 : Hello and Welcome to Webinar Participants!\n node 14 : Hello and Welcome to Webinar Participants!\n node 10 : Hello and Welcome to Webinar Participants!\n node 13 : Hello and Welcome to Webinar Participants!\n node 15 : Hello and Welcome to Webinar Participants!\n node 9 : Hello and Welcome to Webinar Participants!\n node 12 : Hello and Welcome to Webinar Participants!\n node 4 : Hello and Welcome to Webinar Participants!\n node 23 : Hello and Welcome to Webinar Participants!\n node 7 : Hello and Welcome to Webinar Participants!\n node 11 : Hello and Welcome to Webinar Participants!\n node 8 : Hello and Welcome to Webinar Participants!\n node 1 : Hello and Welcome to Webinar Participants!\n node 2 : Hello and Welcome to Webinar Participants!\n node 0 : Hello and Welcome to Webinar Participants!\n node 39 : Hello and Welcome to Webinar Participants!\n node 38 : Hello and Welcome to Webinar Participants!\n node 47 : Hello and Welcome to Webinar Participants!\n node 45 : Hello and Welcome to Webinar Participants!\n node 42 : Hello and Welcome to Webinar Participants!\n node 35 : Hello and Welcome to Webinar Participants!\n node 28 : Hello and Welcome to Webinar Participants!\n node 32 : Hello and Welcome to Webinar Participants!\n node 40 : Hello and Welcome to Webinar Participants!\n node 44 : Hello and Welcome to Webinar Participants!\n node 41 : Hello and Welcome to Webinar Participants!\n node 30 : Hello and Welcome to Webinar Participants!\n node 31 : Hello and Welcome to Webinar Participants!\n node 29 : Hello and Welcome to Webinar Participants!\n node 37 : Hello and Welcome to Webinar Participants!\n node 43 : Hello and Welcome to Webinar Participants!\n node 46 : Hello and Welcome to Webinar Participants!\n node 34 : Hello and Welcome to Webinar Participants!\n node 26 : Hello and Welcome to Webinar Participants!\n node 24 : Hello and Welcome to Webinar Participants!\n node 27 : Hello and Welcome to Webinar Participants!\n node 25 : Hello and Welcome to Webinar Participants!\n node 33 : Hello and Welcome to Webinar Participants!\n node 36 : Hello and Welcome to Webinar Participants!\nIBRUN: Job ended with value 0\n[mthomas@comet-ln3 IBRUN]$\n```\n* Note the order in which the output was written into the output file. There is an entry for each of the 48 cores (2 nodes, 24 cores/node), but the output is not ordered. This is typical because the time for each core to start and finish its work is asynchronous.\n\n[Back to CPU Jobs](#comp-and-run-cpu-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n### Hello World (OpenMP): <a name=\"hello-world-omp\"></a>\n<b>Subsections:</b>\n* [Hello World (OpenMP): Source Code](#hello-world-omp-source)\n* [Hello World (OpenMP): Compiling](#hello-world-omp-compile)\n* [Hello World (OpenMP): Batch Script Submission](#hello-world-omp-batch-submit)\n* [Hello World (OpenMP): Batch Script Output](#hello-world-omp-batch-output)\n\n\n#### Hello World (OpenMP): Source Code <a name=\"hello-world-omp-source\"></a>\n\nChange to the OPENMP examples directory:\n```\n[mthomas@comet-ln3 comet101]$ cd OPENMP/\n[mthomas@comet-ln3 OPENMP]$ ls -al\ntotal 479\ndrwxr-xr-x 2 username use300 6 Aug 5 22:19 .\ndrwxr-xr-x 16 username use300 16 Aug 5 19:02 ..\n-rwxr-xr-x 1 username use300 728112 Aug 5 19:02 hello_openmp\n-rw-r--r-- 1 username use300 267 Aug 5 22:19 hello_openmp.f90\n-rw-r--r-- 1 username use300 310 Aug 5 19:02 openmp-slurm.sb\n-rw-r--r-- 1 username use300 347 Aug 5 19:02 openmp-slurm-shared.sb\n\n[mthomas@comet-ln3 OPENMP]$ cat hello_openmp.f90\n PROGRAM OMPHELLO\n INTEGER TNUMBER\n INTEGER OMP_GET_THREAD_NUM\n\n!$OMP PARALLEL DEFAULT(PRIVATE)\n TNUMBER = OMP_GET_THREAD_NUM()\n PRINT *, 'Hello from Thread Number[',TNUMBER,'] and Welcome Webinar!'\n!$OMP END PARALLEL\n\n STOP\n END\n```\n[Back to CPU Jobs](#comp-and-run-cpu-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n#### Hello World (OpenMP): Compiling: <a name=\"hello-world-omp-compile\"></a>\n\nNote that there is already a compiled version of the `hello_openmp.f90` code. You can save or delete this version.\n\n* In this example, we compile the source code using the `ifort` command, and verify that it was created:\n```\n[mthomas@comet-ln3 OPENMP]$ ifort -o hello_openmp -qopenmp hello_openmp.f90\n[mthomas@comet-ln3 OPENMP]$ ls -al\n[mthomas@comet-ln3:~/comet101/OPENMP] ll\ntotal 77\ndrwxr-xr-x 2 mthomas use300 7 Apr 16 00:35 .\ndrwxr-xr-x 6 mthomas use300 6 Apr 15 20:10 ..\n-rwxr-xr-x 1 mthomas use300 816952 Apr 16 00:35 hello_openmp\n-rw-r--r-- 1 mthomas use300 267 Apr 15 15:47 hello_openmp_2.f90\n-rw-r--r-- 1 mthomas use300 267 Apr 15 15:47 hello_openmp.f90\n-rw-r--r-- 1 mthomas use300 311 Apr 15 15:47 openmp-slurm.sb\n-rw-r--r-- 1 mthomas use300 347 Apr 15 15:47 openmp-slurm-shared.sb\n\n```\n* Note that if you try to run OpenMP code from the command line, in the current environment, the code will run (because it is based on Pthreads, which exist on the node):\n```\n[mthomas@comet-ln2 OPENMP]$ ./hello_openmp\nHello from Thread Number[ 8 ] and Welcome HPC Trainees!\nHello from Thread Number[ 3 ] and Welcome HPC Trainees!\nHello from Thread Number[ 16 ] and Welcome HPC Trainees!\nHello from Thread Number[ 12 ] and Welcome HPC Trainees!\nHello from Thread Number[ 9 ] and Welcome HPC Trainees!\nHello from Thread Number[ 5 ] and Welcome HPC Trainees!\nHello from Thread Number[ 4 ] and Welcome HPC Trainees!\nHello from Thread Number[ 14 ] and Welcome HPC Trainees!\nHello from Thread Number[ 7 ] and Welcome HPC Trainees!\nHello from Thread Number[ 11 ] and Welcome HPC Trainees!\nHello from Thread Number[ 13 ] and Welcome HPC Trainees!\nHello from Thread Number[ 6 ] and Welcome HPC Trainees!\nHello from Thread Number[ 10 ] and Welcome HPC Trainees!\nHello from Thread Number[ 19 ] and Welcome HPC Trainees!\nHello from Thread Number[ 15 ] and Welcome HPC Trainees!\nHello from Thread Number[ 2 ] and Welcome HPC Trainees!\nHello from Thread Number[ 18 ] and Welcome HPC Trainees!\nHello from Thread Number[ 17 ] and Welcome HPC Trainees!\nHello from Thread Number[ 23 ] and Welcome HPC Trainees!\nHello from Thread Number[ 20 ] and Welcome HPC Trainees!\nHello from Thread Number[ 22 ] and Welcome HPC Trainees!\nHello from Thread Number[ 1 ] and Welcome HPC Trainees!\nHello from Thread Number[ 0 ] and Welcome HPC Trainees!\nHello from Thread Number[ 21 ] and Welcome HPC Trainees!\n```\n* In the example below, we used the OpenMP feature to set the number of threads from the command line.\n\n```\n[mthomas@comet-ln3 OPENMP]$ export OMP_NUM_THREADS=4; ./hello_openmp\nHello from Thread Number[ 0 ] and Welcome HPC Trainees!\nHello from Thread Number[ 1 ] and Welcome HPC Trainees!\nHello from Thread Number[ 2 ] and Welcome HPC Trainees!\nHello from Thread Number[ 3 ] and Welcome HPC Trainees!\n ```\n\n[Back to CPU Jobs](#comp-and-run-cpu-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n#### <a name=\"hello-world-omp-batch-submit\"></a>Hello World (OpenMP): Batch Script Submission\nThe submit script is openmp-slurm.sb:\n\n```\n[mthomas@comet-ln2 OPENMP]$ cat openmp-slurm.sb\n#!/bin/bash\n#SBATCH --job-name=\"hello_openmp\"\n#SBATCH --output=\"hello_openmp.%j.%N.out\"\n#SBATCH --partition=compute\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=24\n#SBATCH --export=ALL\n#SBATCH -t 01:30:00\n\n# Define the user environment\nsource /etc/profile.d/modules.sh\nmodule purge\nmodule load intel\nmodule load mvapich2_ib\n\n#SET the number of openmp threads\nexport OMP_NUM_THREADS=24\n\n#Run the job using mpirun_rsh\n./hello_openmp\n```\n* to submit use the sbatch command:\n```\n[mthomas@comet-ln2 OPENMP]$ sbatch openmp-slurm.sb\nSubmitted batch job 32661678\n[mthomas@comet-ln2 OPENMP]$ squeue -u username\n JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n 32661678 compute hello_op mthomas PD 0:00 1 (Priority)\n ...\n```\n\n[Back to CPU Jobs](#comp-and-run-cpu-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n#### Hello World (OpenMP): Batch Script Output: <a name=\"hello-world-omp-batch-output\"></a>\n\n* Once the job is finished:\n```\n[mthomas@comet-ln2 OPENMP] cat hello_openmp.32661678.comet-07-47.out\n Hello from Thread Number[ 5 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 7 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 16 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 9 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 18 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 12 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 10 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 0 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 14 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 4 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 3 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 11 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 19 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 22 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 15 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 2 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 6 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 1 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 21 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 20 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 17 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 23 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 13 ] and Welcome HPC Trainees!\n Hello from Thread Number[ 8 ] and Welcome HPC Trainees!\n ```\n\n[Back to CPU Jobs](#comp-and-run-cpu-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n### Hybrid (MPI + OpenMP) Jobs: <a name=\"hybrid-mpi-omp\"></a>\n<b>Subsections:</b>\n* [Hybrid (MPI + OpenMP): Source Code](#hybrid-mpi-omp-source)\n* [Hybrid (MPI + OpenMP): Compiling](#hybrid-mpi-omp-compile)\n* [Hybrid (MPI + OpenMP): Batch Script Submission](#hybrid-mpi-omp-batch-submit)\n* [Hybrid (MPI + OpenMP): Batch Script Output](#hybrid-mpi-omp-batch-output)\n\n\n### Hybrid (MPI + OpenMP) Source Code: <a name=\"hybrid-mpi-omp-source\"></a>\n#Several HPC codes use a hybrid MPI, OpenMP approach.\n* `ibrun` wrapper developed to handle such hybrid use cases. Automatically senses the MPI build (mvapich2, openmpi) and binds tasks correctly.\n* `ibrun -help` gives detailed usage info.\n* hello_hybrid.c is a sample code, and hello_hybrid.cmd shows “ibrun” usage.\n* Change to the HYBRID examples directory:\n\n```\n[mthomas@comet-ln2 comet101]$ cd HYBRID/\n[mthomas@comet-ln2 HYBRID]$ ll\ntotal 94\ndrwxr-xr-x 2 username use300 5 Aug 5 19:02 .\ndrwxr-xr-x 16 username use300 16 Aug 5 19:02 ..\n-rwxr-xr-x 1 username use300 103032 Aug 5 19:02 hello_hybrid\n-rw-r--r-- 1 username use300 636 Aug 5 19:02 hello_hybrid.c\n-rw-r--r-- 1 username use300 390 Aug 5 19:02 hybrid-slurm.sb\n```\n* Look at the contents of the `hello_hybrid.c` file\n```\n[mthomas@comet-ln2 HYBRID]$ cat hello_hybrid.c\n#include <stdio.h>\n#include \"mpi.h\"\n#include <omp.h>\n\nint main(int argc, char *argv[]) {\n int numprocs, rank, namelen;\n char processor_name[MPI_MAX_PROCESSOR_NAME];\n int iam = 0, np = 1;\n\n MPI_Init(&argc, &argv);\n MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n MPI_Get_processor_name(processor_name, &namelen);\n\n #pragma omp parallel default(shared) private(iam, np)\n {\n np = omp_get_num_threads();\n iam = omp_get_thread_num();\n printf(\"Hello Webinar participants from thread %d out of %d from process %d out of %d on %s\\n\",\n iam, np, rank, numprocs, processor_name);\n }\n\n MPI_Finalize();\n}\n```\n\n[Back to CPU Jobs](#comp-and-run-cpu-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n#### Hybrid (MPI + OpenMP): Compiling: <a name=\"hybrid-mpi-omp-compile\"></a>\n* To compile the hybrid MPI + OpenMPI code, we need to refer to the table of compilers listed above (and listed in the user guide).\n* We will use the command `mpicx -openmp`\n```\n[mthomas@comet-ln2 HYBRID]$ mpicc -openmp -o hello_hybrid hello_hybrid.c\n[mthomas@comet-ln2 HYBRID]$ ll\ntotal 39\ndrwxr-xr-x 2 username use300 5 Aug 6 00:12 .\ndrwxr-xr-x 16 username use300 16 Aug 5 19:02 ..\n-rwxr-xr-x 1 username use300 103032 Aug 6 00:12 hello_hybrid\n-rw-r--r-- 1 username use300 636 Aug 5 19:02 hello_hybrid.c\n-rw-r--r-- 1 username use300 390 Aug 5 19:02 hybrid-slurm.sb\n\n```\n\n[Back to CPU Jobs](#comp-and-run-cpu-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n\n#### Hybrid (MPI + OpenMP): Batch Script Submission: <a name=\"hybrid-mpi-omp-batch-submit\"></a>\n* To submit the hybrid code, we still use the `ibrun` command.\n* In this example, we set the number of threads explicitly.\n```\n[mthomas@comet-ln2 HYBRID]$ cat hybrid-slurm.sb\n#!/bin/bash\n#SBATCH --job-name=\"hellohybrid\"\n#SBATCH --output=\"hellohybrid.%j.%N.out\"\n#SBATCH --partition=compute\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=24\n#SBATCH --export=ALL\n#SBATCH -t 01:30:00\n\n\n# Define the user environment\nsource /etc/profile.d/modules.sh\nmodule purge\nmodule load intel\nmodule load mvapich2_ib\n\n#This job runs with 2 nodes, 24 cores per node for a total of 48 cores.\n# We use 8 MPI tasks and 6 OpenMP threads per MPI task\n\nexport OMP_NUM_THREADS=6\nibrun --npernode 4 ./hello_hybrid\n```\n* Submit the job to the Slurm queue, and check the job status\n```\n[mthomas@comet-ln2 HYBRID]$ sbatch hybrid-slurm.sb\nSubmitted batch job 18347079\n[mthomas@comet-ln2 HYBRID]$ squeue -u username\n JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n 18347079 compute hellohyb username R 0:04 2 comet-01-[01,04]\n[mthomas@comet-ln2 HYBRID]$ ll\n```\n\n[Back to CPU Jobs](#comp-and-run-cpu-jobs) <br>\n[Back to Top](#top)\n<hr>\n\n\n#### Hybrid (MPI + OpenMP): Batch Script Output: <a name=\"hybrid-mpi-omp-batch-output\"></a>\n\n```\n[mthomas@comet-ln2 HYBRID]$ ll\ntotal 122\ndrwxr-xr-x 2 username use300 6 Aug 6 00:12 .\ndrwxr-xr-x 16 username use300 16 Aug 5 19:02 ..\n-rwxr-xr-x 1 username use300 103032 Aug 6 00:12 hello_hybrid\n-rw-r--r-- 1 username use300 3696 Aug 6 00:12 hellohybrid.18347079.comet-01-01.out\n-rw-r--r-- 1 username use300 636 Aug 5 19:02 hello_hybrid.c\n-rw-r--r-- 1 username use300 390 Aug 5 19:02 hybrid-slurm.sb\n[mthomas@comet-ln2 HYBRID]$ cat hellohybrid.18347079.comet-01-01.out\nHello from thread 4 out of 6 from process 3 out of 8 on comet-01-01.sdsc.edu\nHello from thread 3 out of 6 from process 2 out of 8 on comet-01-01.sdsc.edu\nHello from thread 0 out of 6 from process 1 out of 8 on comet-01-01.sdsc.edu\nHello from thread 2 out of 6 from process 2 out of 8 on comet-01-01.sdsc.edu\nHello from thread 1 out of 6 from process 3 out of 8 on comet-01-01.sdsc.edu\nHello from thread 2 out of 6 from process 1 out of 8 on comet-01-01.sdsc.edu\nHello from thread 4 out of 6 from process 2 out of 8 on comet-01-01.sdsc.edu\nHello from thread 0 out of 6 from process 3 out of 8 on comet-01-01.sdsc.edu\nHello from thread 3 out of 6 from process 1 out of 8 on comet-01-01.sdsc.edu\nHello from thread 5 out of 6 from process 2 out of 8 on comet-01-01.sdsc.edu\nHello from thread 3 out of 6 from process 3 out of 8 on comet-01-01.sdsc.edu\nHello from thread 4 out of 6 from process 1 out of 8 on comet-01-01.sdsc.edu\nHello from thread 0 out of 6 from process 2 out of 8 on comet-01-01.sdsc.edu\nHello from thread 2 out of 6 from process 3 out of 8 on comet-01-01.sdsc.edu\nHello from thread 5 out of 6 from process 1 out of 8 on comet-01-01.sdsc.edu\nHello from thread 5 out of 6 from process 3 out of 8 on comet-01-01.sdsc.edu\nHello from thread 3 out of 6 from process 0 out of 8 on comet-01-01.sdsc.edu\nHello from thread 2 out of 6 from process 0 out of 8 on comet-01-01.sdsc.edu\nHello from thread 0 out of 6 from process 0 out of 8 on comet-01-01.sdsc.edu\nHello from thread 4 out of 6 from process 0 out of 8 on comet-01-01.sdsc.edu\nHello from thread 5 out of 6 from process 0 out of 8 on comet-01-01.sdsc.edu\nHello from thread 1 out of 6 from process 2 out of 8 on comet-01-01.sdsc.edu\nHello from thread 1 out of 6 from process 1 out of 8 on comet-01-01.sdsc.edu\nHello from thread 1 out of 6 from process 0 out of 8 on comet-01-01.sdsc.edu\nHello from thread 0 out of 6 from process 7 out of 8 on comet-01-04.sdsc.edu\nHello from thread 0 out of 6 from process 6 out of 8 on comet-01-04.sdsc.edu\nHello from thread 2 out of 6 from process 7 out of 8 on comet-01-04.sdsc.edu\nHello from thread 2 out of 6 from process 6 out of 8 on comet-01-04.sdsc.edu\nHello from thread 3 out of 6 from process 7 out of 8 on comet-01-04.sdsc.edu\nHello from thread 5 out of 6 from process 6 out of 8 on comet-01-04.sdsc.edu\nHello from thread 4 out of 6 from process 6 out of 8 on comet-01-04.sdsc.edu\nHello from thread 1 out of 6 from process 7 out of 8 on comet-01-04.sdsc.edu\nHello from thread 4 out of 6 from process 7 out of 8 on comet-01-04.sdsc.edu\nHello from thread 1 out of 6 from process 6 out of 8 on comet-01-04.sdsc.edu\nHello from thread 0 out of 6 from process 4 out of 8 on comet-01-04.sdsc.edu\nHello from thread 5 out of 6 from process 4 out of 8 on comet-01-04.sdsc.edu\nHello from thread 2 out of 6 from process 4 out of 8 on comet-01-04.sdsc.edu\nHello from thread 1 out of 6 from process 4 out of 8 on comet-01-04.sdsc.edu\nHello from thread 3 out of 6 from process 4 out of 8 on comet-01-04.sdsc.edu\nHello from thread 4 out of 6 from process 4 out of 8 on comet-01-04.sdsc.edu\nHello from thread 0 out of 6 from process 5 out of 8 on comet-01-04.sdsc.edu\nHello from thread 1 out of 6 from process 5 out of 8 on comet-01-04.sdsc.edu\nHello from thread 4 out of 6 from process 5 out of 8 on comet-01-04.sdsc.edu\nHello from thread 2 out of 6 from process 5 out of 8 on comet-01-04.sdsc.edu\nHello from thread 5 out of 6 from process 5 out of 8 on comet-01-04.sdsc.edu\nHello from thread 3 out of 6 from process 6 out of 8 on comet-01-04.sdsc.edu\nHello from thread 5 out of 6 from process 7 out of 8 on comet-01-04.sdsc.edu\nHello from thread 3 out of 6 from process 5 out of 8 on comet-01-04.sdsc.edu\n[mthomas@comet-ln2 HYBRID]$\n```\n\n[Back to CPU Jobs](#comp-and-run-cpu-jobs) <br>\n[Back to Top](#top)\n<hr>\n","dir":"/comet-101/","name":"running_jobs_on_comet.md","path":"comet-101/running_jobs_on_comet.md","url":"/comet-101/running_jobs_on_comet.html"},{"permalink":"/expanse-101/","layout":"default","title":"Expanse 101 Tutorial","content":"# Expanse 101 Tutorial\n\nSee: [expanse.sdsc.edu](https://expanse.sdsc.edu)\n\n## NOTES:\n\n### Interactive Video Link:\nThis material was presented as part of a webinar presented on October 8th, 2020.\nThe link to the presentation will be posted soon:\n\n[Interactive Video](https://education.sdsc.edu/training/interactive/202009_expanse_101/index.php)\n\n### Hands-on Self-guided Tutorial (using markdown file):\n\nhttps://github.com/sdsc-hpc-training-org/expanse-101/blob/master/running_jobs_on_expanse.md\n\n### Link to GitHub Repo\nhttps://github.com/sdsc-hpc-training-org/expanse-101\n\n### Expanse User Guide\nPlease read the Expanse user guide and familiarize yourself with the hardware, file systems, batch job submission, compilers and modules. The guide can be found here:\n\nhttp://www.sdsc.edu/support/user_guides/expanse.html\n\n### Basic Skills\nYou should be familiar with running basic Unix commands, connecting to Expanse via SSH, and other basic skills. See:\nhttps://github.com/sdsc-hpc-training/basic_skills\n\n","dir":"/expanse-101/","name":"README.md","path":"expanse-101/README.md","url":"/expanse-101/"},{"layout":"default","title":"Managing Accounts on Expanse","content":"## <a name=\"sys-env\"></a>Managing Accounts on Expanse\nThis section focuses on how to manage your allocations on Expanse. For full details, see the [Expanse User Guide.](https://www.sdsc.edu/support/user_guides/expanse.html#accounts)\n\n<a name=\"top\"> In this Section:\n* [Expanse Accounts](#expanse-accounts)\n* [Logging Onto Expanse](#expanse-logon)\n* [Obtaining Example Code](#example-code)\n* [Expanse User Portal](#user-portal)\n\n### Expanse Accounts<a name=\"expanse-accounts\"></a>\nYou must have a expanse account in order to access the system.\n* To obtain a trial account or an expedited allocation, go here: http://www.sdsc.edu/support/user_guides/expanse.html#trial_accounts\n* To obtain a trial account or an expedited allocation, go here: http://www.sdsc.edu/support/user_guides/expanse.html#trial_accounts\n* *Note:* You may need to create an XSEDE portal account. XSEDE portal accounts are open to the general community. However, access and allocations to specific XSEDE or SDSC systems will depend on the details of an allocation request.\n\n### Logging Onto Expanse<a name=\"expanse-logon\"></a>\n\nExpanse supports *Single Sign-On* through the [XSEDE User Portal](https://portal.xsede.org), from the command line using an XSEDE-wide password. While CPU and GPU resources are allocated separately, the ```login``` nodes are the same. To log in to Expanse from the command line, use the hostname:\n\n```\nlogin.expanse.sdsc.edu\n```\n\nThe following are examples of Secure Shell (ssh) commands that may be used to log in to Expanse:\n```\nssh <your_username>@login.expanse.sdsc.edu\nssh -l <your_username> login.expanse.sdsc.edu\n```\n* Details about how to access Expanse under different circumstances are described in the Expanse User Guide: https://www.sdsc.edu/support/user_guides/expanse.html#access\n\n* For instructions on how to use SSH,\nsee our tutorial: [Connecting to SDSC HPC Systems Guide](https://github.com/sdsc-hpc-training-org/hpc-security/tree/master/connecting-to-hpc-systems).\n\n* Below is an example of the logon message – often called the *MOTD* (message of the day, located in /etc/motd). This has not been implemented at this point on Expanse\n\n```\n[username@localhost:~] ssh expanse.sdsc.edu\nWelcome to Bright release 9.0\n\n Based on CentOS Linux 8\n ID: #000002\n\n--------------------------------------------------------------------------------\n\n WELCOME TO\n _______ __ ____ ___ _ _______ ______\n / ____/ |/ // __ \\/ | / | / / ___// ____/\n / __/ | // /_/ / /| | / |/ /\\__ \\/ __/\n / /___ / |/ ____/ ___ |/ /| /___/ / /___\n /_____//_/|_/_/ /_/ |_/_/ |_//____/_____/\n\n--------------------------------------------------------------------------------\n\nUse the following commands to adjust your environment:\n\n'module avail' - show available modules\n'module add <module>' - adds a module to your environment for this session\n'module initadd <module>' - configure module to be loaded at every login\n\n-------------------------------------------------------------------------------\nLast login: Fri Nov 1 11:16:02 2020 from 76.176.117.51\n```\n\n####\nExample of a terminal connection/Unix login session:\n```(base) localhost:~ username$ ssh -l username login.expanse.sdsc.edu\nLast login: Wed Oct  7 11:04:17 2020 from 76.176.117.51\n[username@login02 ~]$ \n[username@login02 ~]$ whoami\nusername\n[username@login02 ~]$ \n[username@login02 ~]$ pwd\n/home/username\n[username@login02 ~]$ \n[username@login02 ~]$ \n```\n\n[Back to Top](#top)\n<hr>\n\n### Obtaining Tutorial Example Code<a name=\"example-code\"></a>\nWe will be clone the example code from GitHub repository located here:\nhttps://github.com/sdsc-hpc-training-org/expanse-101\n\nThe example below will be for anonymous HTTPS downloads\n\n* Create a test directory hold the expanse example files:\n```\nlocalhost:hpctrain username$ git clone https://github.com/sdsc-hpc-training-org/expanse-101.git\nCloning into 'expanse-101'...\nremote: Enumerating objects: 79, done.\nremote: Counting objects: 100% (79/79), done.\nremote: Compressing objects: 100% (52/52), done.\nremote: Total 302 (delta 36), reused 39 (delta 15), pack-reused 223\nReceiving objects: 100% (302/302), 3.70 MiB | 4.66 MiB/s, done.\nResolving deltas: 100% (130/130), done.\nlocalhost:hpctrain username$ ll\ntotal 0\ndrwxr-xr-x 3 username staff 96 Nov 18 08:12 .\ndrwxr-xr-x 11 username staff 352 Nov 18 08:11 ..\ndrwxr-xr-x 10 username staff 320 Nov 18 08:12 expanse-101\nlocalhost:hpctrain username$ cd expanse-101/\nlocalhost:expanse-101 username$ ls -al\ntotal 48\ndrwxr-xr-x 10 username staff 320 Nov 18 08:12 .\ndrwxr-xr-x 3 username staff 96 Nov 18 08:12 ..\n-rw-r--r-- 1 username staff 6148 Nov 18 08:12 .DS_Store\ndrwxr-xr-x 12 username staff 384 Nov 18 08:12 .git\n-rw-r--r-- 1 username staff 459 Nov 18 08:12 .gitignore\n-rw-r--r-- 1 username staff 1005 Nov 18 08:12 README.md\ndrwxr-xr-x 4 username staff 128 Nov 18 08:12 docs\ndrwxr-xr-x 7 username staff 224 Nov 18 08:12 examples\ndrwxr-xr-x 12 username staff 384 Nov 18 08:12 images\n-rw-r--r-- 1 username staff 5061 Nov 18 08:12 running_jobs_on_expanse.md\n```\n*Note*: you can learn to create and modify directories as part of the *Getting Started* and *Basic Skills* preparation found here:\nhttps://github.com/sdsc-hpc-training-org/basic_skills\n\nThe examples directory contains the code we will cover in this tutorial:\n```\n[username@login01 examples]$ ls -al examples\ntotal 88\ndrwxr-xr-x 6 username use300 6 Oct  7 14:15 .\ndrwxr-xr-x 5 username use300 8 Oct  7 14:15 ..\ndrwxr-xr-x 2 username use300 6 Oct  7 14:15 HYBRID\ndrwxr-xr-x 2 username use300 6 Oct  7 14:15 MPI\ndrwxr-xr-x 2 username use300 6 Oct  7 14:15 OpenACC\ndrwxr-xr-x 2 username use300 6 Oct  7 14:15 OPENMP\n[username@login01 examples]$ ls -al examples/MPI\ntotal 63\ndrwxr-xr-x 2 username use300     6 Oct  7 14:15 .\ndrwxr-xr-x 6 username use300     6 Oct  7 14:15 ..\n-rwxr-xr-x 1 username use300 21576 Oct  7 14:15 hello_mpi\n-rw-r--r-- 1 username use300   329 Oct  7 14:15 hello_mpi.f90\n-rw-r--r-- 1 username use300   464 Oct  7 14:15 hellompi-slurm.sb\n-rw-r--r-- 1 username use300   181 Oct  7 14:15 README.txt\n\n```\nAll examples will contain source code, along with a batch script example so you can compile and run all examples on Expanse.\n\n### Expanse User Portal<a name=\"user-portal\"></a>\n<img src=\"../images/expanse_usesr_portal.png\" alt=\"Expanse User Portal\" width=\"400px\" />\n\n* See: https://portal.expanse.sdsc.edu\n* Quick and easy way for Expanse users to login, transfer and edit files and submit and monitor jobs. \n* Gateway for launching interactive applications such as MATLAB, Rstudio\n* Integrated web-based environment for file management and job submission.\n* All Users with valid Expanse Allocation and XSEDE Based credentials have access via their XSEDE credentials..\n\n\n[Back to Top](#top)\n<hr>\n","dir":"/expanse-101/docs/","name":"accounts.md","path":"expanse-101/docs/accounts.md","url":"/expanse-101/docs/accounts.html"},{"layout":"default","title":"Compiling &amp; Linking Code","content":"## <a name=\"compilers\"></a>Compiling & Linking Code\n\n<a name=\"top\"> In this Section:\n* [Compiling & Linking](#compilers)\n * [Supported Compiler Types](#compilers-supported)\n * [Using the AMD Compilers](#compilers-intel)\n * [Using the Intel Compilers](#compilers-intel)\n * [Using the PGI Compilers](#compilers-pgi)\n * [Using the GNU Compilers](#compilers-gnu)\n\n\n### <a name=\"compilers\"></a>Expanse Compilers\n\nExpanse provides the Intel, Portland Group (PGI), and GNU compilers along with multiple MPI implementations (MVAPICH2, MPICH2, OpenMPI). Most applications will achieve the best performance on Expanse using the Intel compilers and MVAPICH2 and the majority of libraries installed on Expanse have been built using this combination.\n\nOther compilers and versions can be installed by Expanse staff on request. For more information, see the [Expanse User Guide.]\n(https://www.sdsc.edu/support/user_guides/expanse.html#compiling)\n\n### <a name=\"compilers-supported\"></a>Supported Compilers\nExpanse CPU and GPU nodes have different compiler libraries.\n\n#### CPU Nodes\n* GNU, Intel, AOCC (AMD) compilers\n* Multiple MPI implementations (OpenMPI, MVAPICH2, and IntelMPI). \n* A majority of applications have been built using gcc/10.2.0 which features AMD Rome specific optimization flags (-march=znver2).\n* Intel, and AOCC compilers all have flags to support Advanced Vector Extensions 2 (AVX2). \n\nUsers should evaluate their application for best compiler and library selection. GNU, Intel, and AOCC compilers all have flags to support Advanced Vector Extensions 2 (AVX2). Using AVX2, up to eight floating point operations can be executed per cycle per core, potentially doubling the performance relative to non-AVX2 processors running at the same clock speed. Note that AVX2 support is not enabled by default and compiler flags must be set as described below.\n\n#### GPU Nodes\nExpanse GPU nodes have GNU, Intel, and PGI compilers available along with multiple MPI implementations (OpenMPI, IntelMPI, and MVAPICH2). The gcc/10.2.0, Intel, and PGI compilers have specific flags for the Cascade Lake architecture. Users should evaluate their application for best compiler and library selections.\n\n*Note: that the login nodes are not the same as the GPU nodes, therefore all GPU codes must be compiled by requesting an interactive session on the GPU nodes.*\n\nIn this tutorial, we include several hands-on examples that cover many of the cases in the table:\n* MPI\n* OpenMP\n* HYBRID\n* GPU\n* Local scratch\n\n[Back to Top](#top)\n<hr>\n\n### <a name=\"compilers-amd\"></a>Using the AMD compilers\n\nThe AMD Optimizing C/C++ Compiler (AOCC) is only available on CPU nodes. AMD compilers can be loaded by executing the following commands at the Linux prompt:\n```\nmodule load aocc\n```\n\nFor more information on the AMD compilers run:\n```\n[flang | clang ] -help\n```\n\nSuggested Compilers to used based on programming model and languages:\n\n|Language | Serial | MPI | OpenMP | MPI + OpenMP |\n| :---- | :---- | :---- | :---- | :---- |\n|Fortran | flang | mpif90 | ifort -fopenmp | mpif90 -fopenmp |\n|C | clang | mpiclang | icc -fopenmp | mpicc -fopenmp |\n| C++ | clang++ | mpiclang | icpc -fopenmp | mpicxx -fopenmp |\n\n[Back to Top](#top)\n<hr>\n\n### <a name=\"compilers-intel\"></a>Using the Intel Compilers:\n\nThe Intel compilers and the MVAPICH2 MPI implementation will be loaded by default. If you have modified your environment, you can reload by executing the following commands at the Linux prompt or placing in your startup file (~/.cshrc or ~/.bashrc) or into a module load script (see above).\n```\nmodule purge\nmodule load intel mvapich2_ib\n```\nFor AVX2 support, compile with the -xHOST option. Note that -xHOST alone does not enable aggressive optimization, so compilation with -O3 is also suggested. The -fast flag invokes -xHOST, but should be avoided since it also turns on interprocedural optimization (-ipo), which may cause problems in some instances.\n\nIntel MKL libraries are available as part of the \"intel\" modules on Expanse. Once this module is loaded, the environment variable MKL_ROOT points to the location of the mkl libraries. The MKL link advisor can be used to ascertain the link line (change the MKL_ROOT aspect appropriately).\n\nIn the example below, we are working with the HPC examples that can be found in\n```\n[user@expanse-14-01:~/expanse-examples/expanse101/MKL] pwd\n/home/user/expanse-examples/expanse101/MKL\n[user@expanse-14-01:~/expanse-examples/expanse101/MKL] ls -al\ntotal 25991\ndrwxr-xr-x 2 user use300 9 Nov 25 17:20 .\ndrwxr-xr-x 16 user use300 16 Aug 5 19:02 ..\n-rw-r--r-- 1 user use300 325 Aug 5 19:02 compile.txt\n-rw-r--r-- 1 user use300 6380 Aug 5 19:02 pdpttr.c\n-rwxr-xr-x 1 user use300 44825440 Nov 25 16:55 pdpttr.exe\n-rw-r--r-- 1 user use300 188 Nov 25 16:57 scalapack.20294236.expanse-07-27.out\n-rw-r--r-- 1 user use300 376 Aug 5 19:02 scalapack.sb\n```\n\nThe file `compile.txt` contains the full command to compile the `pdpttr.c` program statically linking 64 bit scalapack libraries on Expanse:\n```\n[user@expanse-14-01:~/expanse-examples/expanse101/MKL] cat compile.txt\nmpicc -o pdpttr.exe pdpttr.c /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64/libmkl_scalapack_lp64.a -Wl,--start-group /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64/libmkl_intel_lp64.a /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64/libmkl_sequential.a /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64/libmkl_core.a /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64/libmkl_blacs_intelmpi_lp64.a -Wl,--end-group -lpthread -lm -ldl\n```\n\nRun the command:\n```\n[user@expanse-14-01:~/expanse-examples/expanse101/MKL] mpicc -o pdpttr.exe pdpttr.c -I$MKL_ROOT/include ${MKL_ROOT}/lib/intel64/libmkl_scalapack_lp64.a -Wl,--start-group ${MKL_ROOT}/lib/intel64/libmkl_intel_lp64.a ${MKL_ROOT}/lib/intel64/libmkl_core.a ${MKL_ROOT}/lib/intel64/libmkl_sequential.a -Wl,--end-group ${MKL_ROOT}/lib/intel64/libmkl_blacs_intelmpi_lp64.a -lpthread -lm\n```\nFor more information on the Intel compilers run: [ifort | icc | icpc] -help\n\n[Back to Top](#top)\n<hr>\n\n### <a name=\"compilers-pgi\"></a>Using the PGI Compilers\nThe PGI compilers can be loaded by executing the following commands at the Linux prompt or placing in your startup file (~/.cshrc or ~/.bashrc)\n\n```\nmodule purge\nmodule load pgi mvapich2_ib\n```\n\nFor AVX support, compile with -fast\n\nFor more information on the PGI compilers: man [pgf90 | pgcc | pgCC]\n\n| |Serial | MPI | OpenMP | MPI+OpenMP|\n|---|---|---|---|---|\n|pgf90 | mpif90 | pgf90 -mp | mpif90 -mp|\n|C | pgcc | mpicc | pgcc -mp | mpicc -mp|\n|C++ | pgCC | mpicxx | pgCC -mp | mpicxx -mp|\n\n[Back to Top](#top)\n<hr>\n\n### <a name=\"compilers-gnu\"></a>Using the GNU Compilers\nThe GNU compilers can be loaded by executing the following commands at the Linux prompt or placing in your startup files (~/.cshrc or ~/.bashrc)\n```\nmodule purge\nmodule load gnu openmpi_ib\n```\n\nFor AVX support, compile with -mavx. Note that AVX support is only available in version 4.7 or later, so it is necessary to explicitly load the gnu/4.9.2 module until such time that it becomes the default.\n\nFor more information on the GNU compilers: man [gfortran | gcc | g++]\n\n| |Serial | MPI | OpenMP | MPI+OpenMP|\n|---|---|---|---|---|\n|Fortran | gfortran | mpif90 | gfortran -fopenmp | mpif90 -fopenmp|\n|C | gcc | mpicc | gcc -fopenmp | mpicc -fopenmp|\n|C++ | g++ | mpicxx | g++ -fopenmp | mpicxx -fopenmp|\n\n\n[Back to Top](#top)\n<hr>\n","dir":"/expanse-101/docs/","name":"compiling.md","path":"expanse-101/docs/compiling.md","url":"/expanse-101/docs/compiling.html"},{"layout":"default","title":null,"content":"<img src=\"https://github.com/sdsc-hpc-training-org/expanse-101/blob/main/images/expanse_overview.png\" alt=\"Expanse Overview\" width=\"500px\">\n\n## Expanse Overview:\n\n### HPC for the *long tail* of science:\n* Designed by Dell and SDSC delivering 5.16 peak petaflops\n* Designed and operated on the principle that the majority of computational research is performed at modest scale: large number jobs that run for less than 48 hours, but can be computationally intensvie and generate large amounts of data.\n* An NSF-funded system available through the eXtreme Science and Engineering Discovery Environment (XSEDE) program (https://www.xsede.org).\n* Supports interactive computing and science gateways.\n* Will offer Composible Systems and Cloud Bursting.\n\n\n<hr>\n\n<img src=\"https://github.com/sdsc-hpc-training-org/expanse-101/blob/main/images/expanse_heterogeneous_arch.png\" alt=\"Expanse Heterogeneous Architecture\" width=\"500px\">\n\n### System Summary\n\n* 13 SDSC Scalable Compute Units (SSCU)\n* 728 x 2s Standard Compute Nodes\n* 93,184 Compute Cores\n* 200 TB DDR4 Memory\n* 52x 4-way GPU Nodes w/NVLINK\n* 208 V100s | * 4x 2TB Large Memory Nodes\n* HDR 100 non-blocking Fabric\n* 12 PB Lustre High Performance\n* Storage\n * 7 PB Ceph Object Storage\n * 1.2 PB on-node NVMe\n* Dell EMC PowerEdge\n* Direct Liquid Cooled\n\n\n<hr>\n\n### Expanse Scaleable Compute Unit\n\n<img src=\"https://github.com/sdsc-hpc-training-org/expanse-101/blob/main/images/expanse_sccu.png\" alt=\"Expanse Scaleable Compute Unit\" width=\"700px\">\n\n<hr>\n\n### Expanse Connectivity Fabric\n\n<img src=\"https://github.com/sdsc-hpc-training-org/expanse-101/blob/main/images/expanse_connectivity_fabric.png\" alt=\"Expanse Connectivity Fabric\" width=\"700px\" />\n\n<hr>\n\n### AMD EPYC 7742 Processor Architecture\n<img src=\"https://github.com/sdsc-hpc-training-org/expanse-101/blob/main/images/amd-epyc-7742-processor-arch.png\" alt=\"AMD EPYC 7742 Processor Architecture\" width=\"300px\" />\n\n\n* 8 Core Complex Dies (CCDs).\n* CCDs connect to memory, I/O, and each other through the I/O Die.\n* 8 memory channels per socket.\n* DDR4 memory at 3200MHz.\n* PCI Gen4, up to 128 lanes of high speed I/O.\n* Memory and I/O can be abstracted into separate quadrants each with 2 DIMM channels and 32 I/O lanes.\n* 2 Core Complexes (CCXs) per CCD\n* 4 Zen2 cores in each CCX share a16ML3 cache. Total of 16x16=256MB L3 cache.\n* Each core includes a private 512KB L2 cache. \n\n<hr>\n\n### New Expanse Feature: Composable Systems\n\n<img src=\"https://github.com/sdsc-hpc-training-org/expanse-101/blob/main/images/expanse_composable_systems.png\" alt=\"Expanse Composable Systems\" width=\"400px\" />\nComposable Systems will support complex, distributed, workflows – making Expanse part of a larger CI ecosystem.\n\n* Bright Cluster Manager + Kubernetes\n* Core components developed via NSF- funded CHASE-CI (NSF Award # 1730158), and the Pacific Research Platform (NSF Award # 1541349)\n* Requests for a composable system will be part of an XRAC request\n* Advanced User Support resources available to assist with projects - this is part of our operations funding.\n * Webinar scheduled for April 2021. See: https://www.sdsc.edu/education_and_training/training_hpc.html\n\n\n### New Expanse Feature: Cloud Bursting\nExpanse will support integration with public clouds:\n\n <img src=\"https://github.com/sdsc-hpc-training-org/expanse-101/blob/main/images/expanse_cloud_burst_aws.png\" alt=\"Expanse Cloud Bursting to AWS\" width=\"400px\" />\n\n* Supports projects that share data, need access to novel technologies, and integrate cloud resources into workflows\n* Slurm + in-house developed software + Terraform (Hashicorp)\n* Early work funded internally and via NSF E-CAS/Internet2 project for CIPRES (Exploring Cloud for the Acceleration of Science, Award #1904444).\n* Approach is cloud-agnostic and will support the major cloud providers.\n* Users submit directly via Slurm, or as part of a composed system.\n* Options for data movement: data in the cloud; remote mounting of file systems; cached filesystems (e.g., StashCache), and data transfer during the job.\n* Funding for users cloud resources is not part of an Expanse award: the researcher must have access to cloud computing credits via other NSF awards and funding.\n[Back to Top](#top)\n<hr>\n","dir":"/expanse-101/docs/","name":"expanse_overview.md","path":"expanse-101/docs/expanse_overview.md","url":"/expanse-101/docs/expanse_overview.html"},{"layout":"default","title":"Getting Started on Expanse","content":"## <a name=\"sys-env\"></a>Getting Started on Expanse\n\n<a name=\"top\"> In this Section:\n* [Expanse Accounts](#expanse-accounts)\n* [Logging Onto Expanse](#expanse-logon)\n* [Obtaining Example Code](#example-code)\n* [Expanse User Portal](#user-portal)\n\n### Expanse Accounts<a name=\"expanse-accounts\"></a>\nYou must have a expanse account in order to access the system.\n* Obtain a trial account here: http://www.sdsc.edu/support/user_guides/expanse.html#trial_accounts\n* You can use your XSEDE account.\n\n\n### Logging Onto Expanse<a name=\"expanse-logon\"></a>\n\nExpanse supports Single Sign-On through the [XSEDE User Portal](https://portal.xsede.org), from the command line using an XSEDE-wide password (coming soon, the Expanse User Portal). While CPU and GPU resources are allocated separately, the login nodes are the same. To log in to Expanse from the command line, use the hostname:\n\n```\nlogin.expanse.sdsc.edu\n```\n\nThe following are examples of Secure Shell (ssh) commands that may be used to log in to Expanse:\n```\nssh <your_username>@login.expanse.sdsc.edu\nssh -l <your_username> login.expanse.sdsc.edu\n```\nDetails about how to access Expanse under different circumstances are described in the Expanse User Guide:\nhttps://www.sdsc.edu/support/user_guides/expanse.html#access\n\nFor instructions on how to use SSH,\nsee [Connecting to SDSC HPC Systems Guide](https://github.com/sdsc-hpc-training-org/hpc-security/tree/master/connecting-to-hpc-systems). Below is the logon message – often called the *MOTD* (message of the day, located in /etc/motd). This has not been implemented at this point on Expanse\n\n```\n[username@localhost:~] ssh -Y expanse.sdsc.edu\nWelcome to Bright release 9.0\n\n Based on CentOS Linux 8\n ID: #000002\n\n--------------------------------------------------------------------------------\n\n WELCOME TO\n _______ __ ____ ___ _ _______ ______\n / ____/ |/ // __ \\/ | / | / / ___// ____/\n / __/ | // /_/ / /| | / |/ /\\__ \\/ __/\n / /___ / |/ ____/ ___ |/ /| /___/ / /___\n /_____//_/|_/_/ /_/ |_/_/ |_//____/_____/\n\n--------------------------------------------------------------------------------\n\nUse the following commands to adjust your environment:\n\n'module avail' - show available modules\n'module add <module>' - adds a module to your environment for this session\n'module initadd <module>' - configure module to be loaded at every login\n\n-------------------------------------------------------------------------------\nLast login: Fri Nov 1 11:16:02 2020 from 76.176.117.51\n```\n\n####\nExample of a terminal connection/Unix login session:\n```(base) localhost:~ username$ ssh -l username login.expanse.sdsc.edu\nLast login: Wed Oct  7 11:04:17 2020 from 76.176.117.51\n[username@login02 ~]$ \n[username@login02 ~]$ whoami\nusername\n[username@login02 ~]$ \n[username@login02 ~]$ pwd\n/home/username\n[username@login02 ~]$ \n[username@login02 ~]$ \n```\n\n[Back to Top](#top)\n<hr>\n\n### Obtaining Tutorial Example Code<a name=\"example-code\"></a>\nWe will be clone the example code from GitHub repository located here:\nhttps://github.com/sdsc-hpc-training-org/expanse-101\n\nThe example below will be for anonymous HTTPS downloads\n\n* Create a test directory hold the expanse example files:\n```\nlocalhost:hpctrain username$ git clone https://github.com/sdsc-hpc-training-org/expanse-101.git\nCloning into 'expanse-101'...\nremote: Enumerating objects: 79, done.\nremote: Counting objects: 100% (79/79), done.\nremote: Compressing objects: 100% (52/52), done.\nremote: Total 302 (delta 36), reused 39 (delta 15), pack-reused 223\nReceiving objects: 100% (302/302), 3.70 MiB | 4.66 MiB/s, done.\nResolving deltas: 100% (130/130), done.\nlocalhost:hpctrain username$ ll\ntotal 0\ndrwxr-xr-x 3 username staff 96 Nov 18 08:12 .\ndrwxr-xr-x 11 username staff 352 Nov 18 08:11 ..\ndrwxr-xr-x 10 username staff 320 Nov 18 08:12 expanse-101\nlocalhost:hpctrain username$ cd expanse-101/\nlocalhost:expanse-101 username$ ls -al\ntotal 48\ndrwxr-xr-x 10 username staff 320 Nov 18 08:12 .\ndrwxr-xr-x 3 username staff 96 Nov 18 08:12 ..\n-rw-r--r-- 1 username staff 6148 Nov 18 08:12 .DS_Store\ndrwxr-xr-x 12 username staff 384 Nov 18 08:12 .git\n-rw-r--r-- 1 username staff 459 Nov 18 08:12 .gitignore\n-rw-r--r-- 1 username staff 1005 Nov 18 08:12 README.md\ndrwxr-xr-x 4 username staff 128 Nov 18 08:12 docs\ndrwxr-xr-x 7 username staff 224 Nov 18 08:12 examples\ndrwxr-xr-x 12 username staff 384 Nov 18 08:12 images\n-rw-r--r-- 1 username staff 5061 Nov 18 08:12 running_jobs_on_expanse.md\n```\n*Note*: you can learn to create and modify directories as part of the *Getting Started* and *Basic Skills* preparation found here:\nhttps://github.com/sdsc-hpc-training-org/basic_skills\n\nThe examples directory contains the code we will cover in this tutorial:\n```\n[username@login01 examples]$ ls -al examples\ntotal 88\ndrwxr-xr-x 6 username use300 6 Oct  7 14:15 .\ndrwxr-xr-x 5 username use300 8 Oct  7 14:15 ..\ndrwxr-xr-x 2 username use300 6 Oct  7 14:15 HYBRID\ndrwxr-xr-x 2 username use300 6 Oct  7 14:15 MPI\ndrwxr-xr-x 2 username use300 6 Oct  7 14:15 OpenACC\ndrwxr-xr-x 2 username use300 6 Oct  7 14:15 OPENMP\n[username@login01 examples]$ ls -al examples/MPI\ntotal 63\ndrwxr-xr-x 2 username use300     6 Oct  7 14:15 .\ndrwxr-xr-x 6 username use300     6 Oct  7 14:15 ..\n-rwxr-xr-x 1 username use300 21576 Oct  7 14:15 hello_mpi\n-rw-r--r-- 1 username use300   329 Oct  7 14:15 hello_mpi.f90\n-rw-r--r-- 1 username use300   464 Oct  7 14:15 hellompi-slurm.sb\n-rw-r--r-- 1 username use300   181 Oct  7 14:15 README.txt\n\n```\nAll examples will contain source code, along with a batch script example so you can compile and run all examples on Expanse.\n\n### Expanse User Portal<a name=\"user-portal\"></a>\n<img src=\"../images/expanse_usesr_portal.png\" alt=\"Expanse User Portal\" width=\"400px\" />\n\n* See: https://portal.expanse.sdsc.edu\n* Quick and easy way for Expanse users to login, transfer and edit files and submit and monitor jobs. \n* Gateway for launching interactive applications such as MATLAB, Rstudio\n* Integrated web-based environment for file management and job submission.\n* All Users with valid Expanse Allocation and XSEDE Based credentials have access via their XSEDE credentials..\n\n\n[Back to Top](#top)\n<hr>\n","dir":"/expanse-101/docs/","name":"getting_started.md","path":"expanse-101/docs/getting_started.md","url":"/expanse-101/docs/getting_started.html"},{"layout":"default","title":"Hands-on Examples","content":"\n## <a name=\"hands-on\"></a>Hands-on Examples\n* [Compiling and Running GPU/CUDA Jobs](#comp-and-run-cuda-jobs)\n* [GPU Hello World (GPU) ](#hello-world-gpu)\n* [GPU Enumeration ](#enum-gpu)\n* [CUDA Mat-Mult](#mat-mul-gpu)\n* [Compiling and Running CPU Jobs](#comp-and-run-cpu-jobs)\n* [Hello World (MPI)](#hello-world-mpi)\n* [Hello World (OpenMPI)](#hello-world-omp)\n* [Compiling and Running Hybrid (MPI + OpenMP) Jobs](#hybrid-mpi-omp)\n","dir":"/expanse-101/docs/","name":"hands_on_examples.md","path":"expanse-101/docs/hands_on_examples.md","url":"/expanse-101/docs/hands_on_examples.html"},{"layout":"default","title":"Expanse Environment Modules: Customizing Your User Environment","content":"## <a name=\"modules\"></a>Expanse Environment Modules: Customizing Your User Environment\nThe Environment Modules package provides for dynamic modification of your shell environment. Module commands set, change, or delete environment variables, typically in support of a particular application. They also let the user choose between different versions of the same software or different combinations of related codes. See the [Expanse User Guide](https://www.sdsc.edu/support/user_guides/expanse.html#modules).\n\n<a name=\"top\"> In this Section:\n* [Introduction to the Lua Lmod Module System](#module-lmod-intro)\n* [Common module commands](#module-commands)\n<!----\n* [Load and Check Modules and Environment](#load-and-check-module-env)\n* [Module Error: command not found](#module-error)\n---->\n\n### Introduction to the Lua Lmod Module System<a name=\"module-lmod-intro\"></a>\n* Expanse uses Lmod, a Lua based module system.\n * See: https://lmod.readthedocs.io/en/latest/010_user.html\n* Users setup custom environments by loading available modules into the shell environment, including needed compilers and libraries and the batch scheduler. \n* What’s the same as Comet:\n * Dynamic modification of your shell environment\n * User can set, change, or delete environment variables\n * User chooses between different versions of the same software or different combinations of related codes.\n* Modules: What’s Different?\n * *Users will need to load the scheduler (e.g. slurm)*\n * Users will not see all available modules when they run command \"module available\" without loading a compiler.\n * Use the command \"module spider\" option to see if a particular package exists and can be loaded, run command\n * module spider <package>\n * module keywords <term>\n * For additional details, and to identify module dependencies modules, use the command\n * module spider <application_name>\n * The module paths are different for the CPU and GPU nodes. Users can enable the paths by loading the following modules:\u000b \n * module load cpu (for cpu nodes)\n * module load gpu (for gpu nodes) \n * note: avoid loading both modules\n\n### Modules: Popular Lmod Commands<a name=\"module-commands\"></a>\n\n\nHere are some common module commands and their descriptions:\n\n| Lmod Command | Description |\n|:--- | :--- |\n|module list|List the modules that are currently loaded|\n|module avail|List the modules that are available in environment|\n|module spider|List of the modules and extensions currently available|\n|module display <module_name>|Show the environment variables used by\n<module name> and how they are affected|\n|module unload <module name>|Remove <module name> from the environment|\n|module load <module name>|Load <module name> into the environment|\n|module swap <module one> <module two>|Replace <module one> with\n<module two> in the environment|\n|module help|get a list of all the commands that module knows about do:\n\nLmod commands support *short-hand* notation, for example:\n```\n ml foo == module load foo\n ml -bar”  == module unload bar\n```\n*SDSC Guidance: add module calls to your environment and batch scripts*\n\n\n\n<b> A few module command examples:</b>\n\n* Default environment: `list`, `li`\n```\n(base) [username@login01 expanse-101]$ module list\nCurrently Loaded Modules:\n 1) shared 2) cpu/1.0 3) DefaultModules\n```\n* List available modules: `available`, `avail`, `av`\n\n```\n$ module av\n[username@expanse-ln3:~] module av\n(base) [username@login01 expanse-101]$ module available\n\n--------------- /cm/shared/apps/spack/cpu/lmod/linux-centos8-x86_64/Core ----------------\n abaqus/2018 gaussian/16.C.01 gmp/6.1.2 mpfr/4.0.2\n aocc/2.2.0 gcc/7.5.0 intel/19.1.1.217 openjdk/11.0.2\n cmake/3.18.2 gcc/9.2.0 libtirpc/1.2.6 parallel/20200822\n emboss/6.6.0 gcc/10.2.0 (D) matlab/2020b subversion/1.14.0\n\n--------------------------------- /cm/local/modulefiles ---------------------------------\n cluster-tools-dell/9.0 gcc/9.2.0 null\n cluster-tools/9.0 gpu/1.0 openldap\n cmd ipmitool/1.8.18 python3\n cmjob kubernetes/expanse/1.18.8 python37\n cpu/1.0 (L) lua/5.3.5 shared (L)\n docker/19.03.5 luajit singularitypro/3.5\n dot module-git slurm/expanse/20.02.3\n freeipmi/1.6.4 module-info\n\n-------------------------------- /usr/share/modulefiles ---------------------------------\n DefaultModules (L) gct/6.2 globus/6.0\n\n-------------------------------- /cm/shared/modulefiles ---------------------------------\n bonnie++/1.98 default-environment netperf/2.7.0\n cm-pmix3/3.1.4 gdb/8.3.1 openblas/dynamic/0.3.7\n cuda10.2/blas/10.2.89 hdf5/1.10.1 openmpi/gcc/64/1.10.7\n cuda10.2/fft/10.2.89 hdf5_18/1.8.21 sdsc/1.0\n cuda10.2/nsight/10.2.89 hwloc/1.11.11 ucx/1.6.1\n cuda10.2/profiler/10.2.89 iozone/3_487\n cuda10.2/toolkit/10.2.89 netcdf/gcc/64/gcc/64/4.7.3\n\n Where:\n L: Module is loaded\n D: Default Module\n\n```\n*Note:* Module defaults are chosen based on Find First Rules due to Name/Version/Version modules found in the module tree.\nSee https://lmod.readthedocs.io/en/latest/060_locating.html for details.\n\nUse ```module spider``` to find all possible modules and extensions.\n```\n(base) [username@login02 ~]$ module spider MPI\n-------------------------------------------------------------------------------------\n intel-mpi: intel-mpi/2019.8.254\n-------------------------------------------------------------------------------------\n You will need to load all module(s) on any one of the lines below before the \"intel-mpi/2019.8.254\" module is available to load.\n\n cpu/1.0 gcc/10.2.0\n cpu/1.0 intel/19.1.1.217\n gpu/1.0\n gpu/1.0 intel/19.0.5.281\n gpu/1.0 pgi/20.4\n\n Help:\n Intel MPI\n-------------------------------------------------------------------------------------\n openmpi:\n-------------------------------------------------------------------------------------\n Versions:\n openmpi/3.1.6\n openmpi/4.0.4-nocuda\n openmpi/4.0.4\n-------------------------------------------------------------------------------------\n For detailed information about a specific \"openmpi\" package (including how to load the modules) use the module's full name. Note that names that have a trailing (E) are extensions provided by other modules.\n For example:\n\n $ module spider openmpi/4.0.4\n-------------------------------------------------------------------------------------\n-------------------------------------------------------------------------------------\n openmpi/gcc/64: openmpi/gcc/64/1.10.7\n-------------------------------------------------------------------------------------\n You will need to load all module(s) on any one of the lines below before the \"openmpi/gcc/64/1.10.7\" module is available to load.\n shared\n Help:\n Adds OpenMPI to your environment variables, \n```\n\n[Back to Top](#top)\n<hr>\n\n### <a name=\"load-and-check-module-env\"></a>Load and Check Modules and Environment\nIn this example, we will add the SLURM library, and and verify that it is in your environment\n* Check login module environment\n```\n(base) [username@login01 ~]$ module li\n\nCurrently Loaded Modules:\n 1) shared 2) cpu/1.0 3) DefaultModules\n```\n\n* Note that SLURM is not in the environment. Check environment looking for SLURM commands\n```\n(base) [username@login01 ~]$ which squeue\n/usr/bin/which: no squeue in (/home/username/miniconda3/bin/conda:/home/username/miniconda3/bin:/home/username/miniconda3/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/dell/srvadmin/bin:/home/username/.local/bin:/home/username/bin)\n```\n* SLURM commands do not exist, so we need to load that module.\n```\n(base) [username@login01 ~]$ module load slurm\n(base) [username@login01 ~]$ which squeue\n/cm/shared/apps/slurm/current/bin/squeue\n```\n\n* Display loaded module details:\n```\n(base) [username@login02 ~]$ module display slurm\n-------------------------------------------------------------------------------------\n /cm/local/modulefiles/slurm/expanse/20.02.3:\n-------------------------------------------------------------------------------------\nwhatis(\"Adds Slurm to your environment \")\nsetenv(\"CMD_WLM_CLUSTER_NAME\",\"expanse\")\nsetenv(\"SLURM_CONF\",\"/cm/shared/apps/slurm/var/etc/expanse/slurm.conf\")\nprepend_path(\"PATH\",\"/cm/shared/apps/slurm/current/bin\")\nprepend_path(\"PATH\",\"/cm/shared/apps/slurm/current/sbin\")\nprepend_path(\"MANPATH\",\"/cm/shared/apps/slurm/current/man\")\nprepend_path(\"LD_LIBRARY_PATH\",\"/cm/shared/apps/slurm/current/lib64\")\nprepend_path(\"LD_LIBRARY_PATH\",\"/cm/shared/apps/slurm/current/lib64/slurm\")\nprepend_path(\"LIBRARY_PATH\",\"/cm/shared/apps/slurm/current/lib64\")\nprepend_path(\"LIBRARY_PATH\",\"/cm/shared/apps/slurm/current/lib64/slurm\")\nprepend_path(\"CPATH\",\"/cm/shared/apps/slurm/current/include\")\nhelp([[ Adds Slurm to your environment\n]])\n```\n\nOnce you have loaded the modules, you can check the system variables that are available for you to use.\n* To see all variable, run the <b>`env`</b> command. Typically, you will see more than 60 lines containing information such as your login name, shell, your home directory:\n```\n[username@expanse-ln3 IBRUN]$ env\nCONDA_EXE=/home/username/miniconda3/bin/conda\n__LMOD_REF_COUNT_PATH=/cm/shared/apps/slurm/current/sbin:1;/cm/shared/apps/slurm/current/bin:1;/home/username/miniconda3/bin/conda:1;/home/username/miniconda3/bin:1;/home/username/miniconda3/condabin:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1;/opt/dell/srvadmin/bin:1;/home/username/.local/bin:1;/home/username/bin:1\nHOSTNAME=login02\nUSER=username\nHOME=/home/username\nCONDA_PYTHON_EXE=/home/username/miniconda3/bin/python\nBASH_ENV=/usr/share/lmod/lmod/init/bash\nBASHRC_READ=1\nLIBRARY_PATH=/cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\nSLURM_CONF=/cm/shared/apps/slurm/var/etc/expanse/slurm.conf\nLOADEDMODULES=shared:cpu/1.0:DefaultModules:slurm/expanse/20.02.3\n__LMOD_REF_COUNT_MANPATH=/cm/shared/apps/slurm/current/man:1;/usr/share/lmod/lmod/share/man:1;/usr/local/. . . .\nMANPATH=/cm/shared/apps/slurm/current/man:/usr/share/lmod/lmod/share/man:/usr/local/share/man:/usr/share/man:/cm/local/apps/environment-modules/current/share/man\nMODULEPATH=/cm/shared/apps/spack/cpu/lmod/linux-centos8-x86_64/Core:/cm/local/modulefiles:/etc/modulefiles:/usr/share/modulefiles:/usr/share/Modules/modulefiles:/cm/shared/modulefiles\nMODULEPATH_ROOT=/usr/share/modulefiles\nPATH=/cm/shared/apps/slurm/current/sbin:/cm/shared/apps/slurm/current/bin:/home/username/miniconda3/bin/conda:/home/username/miniconda3/bin:/home/username/miniconda3/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/dell/srvadmin/bin:/home/username/.local/bin:/home/username/bin\n_LMFILES_=/cm/local/modulefiles/shared:/cm/local/modulefiles/cpu/1.0.lua:/usr/share/modulefiles/DefaultModules.lua:/cm/local/modulefiles/slurm/expanse/20.02.3\nMODULESHOME=/usr/share/lmod/lmod\nCONDA_DEFAULT_ENV=base\n\n```\n\nTo see the value for any of these variables, use the `echo` command:\n```\nxxx\n```\n[Back to Top](#top)\n<hr>\n\n### Loading Modules During Login <a name=\"module-login-load\"></a>\nYou can override, and add to the standard set of login modules in two ways.\n1. The first is adding module commands to your personal startup files.\n2. The second way is through the “module save” command.\n*Note: make sure that you always want the module loaded at login*\n\nFor Bash: put the following block into your ```~/.bash_profile``` file:\n```\n if [ -f ~/.bashrc ]; then\n . ~/.bashrc\n fi\n```\nPlace the following in your ```~/.bashrc``` file:\n```\n if [ -z \"$BASHRC_READ\" ]; then\n export BASHRC_READ=1\n # Place any module commands here\n # module load hdf5\n fi\n```\n\n* First edit your ```.bashrc``` and ```.bash_profile``` files:\n```\n[username@login02 ~]$ cat .bash_profile \n# .bash_profile\n# Get the aliases and functions\nif [ -f ~/.bashrc ]; then\n. ~/.bashrc\nfi\n[SNIP]\n[username@login01 ~]$\n[username@login02 ~]$ cat .bashrc\n# .bashrc\n# Source global definitions\nif [ -f /etc/bashrc ]; then\n. /etc/bashrc\nfi\nif [ -z \"$BASHRC_READ\" ]; then\n   export BASHRC_READ=1\n   # Place any module commands here\n   module load hdf5\nFi\n[SNIP]\n```\n* Next LOGOUT and LOG BACK IN:\n```\n(base) [username@login02 ~]$ env | grep slurm\n[snip]\nMANPATH=/cm/shared/apps/slurm/current/man:/usr/share/lmod/lmod/share/man:/usr/local/share/man:/usr/share/man:/cm/local/apps/environment-modules/current/share/man\nPATH=/cm/shared/apps/slurm/current/sbin:/cm/shared/apps/slurm/current/bin:/home/username/miniconda3/bin/conda:/home/username/miniconda3/bin:/home/username/miniconda3/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/dell/srvadmin/bin:/home/username/.local/bin:/home/username/bin\n[snip]\n(base) [username@login02 ~]$ which squeue\n/cm/shared/apps/slurm/current/bin/squeue\n```\n\n\n### Troubleshooting:Module Error<a name=\"module-error\"></a>\n\nSometimes this error is encountered when switching from one shell to another or attempting to run the module command from within a shell script or batch job. The module command may not be inherited between the shells. To keep this from happening, execute the following command:\n```\n[expanse-ln3:~]source /etc/profile.d/modules.sh\n```\nOR add this command to your shell script (including Slurm batch scripts)\n\n\n[Back to Top](#top)\n<hr>\n","dir":"/expanse-101/docs/","name":"modules.md","path":"expanse-101/docs/modules.md","url":"/expanse-101/docs/modules.html"},{"layout":"default","title":"Expanse 101: Introduction to Running Jobs on the Expanse Supercomputer","content":"# Expanse 101: Introduction to Running Jobs on the Expanse Supercomputer\n\n## Presented by:\n*Mary Thomas (SDSC, mpthomas@ucsd.edu )*\n\n*Thursday, October 8, 2020*\n\n<hr>\nIn this tutorial, you will learn how to compile and run jobs on Expanse,\nwhere to run them, and how to run batch jobs. The commands below can be\ncut & pasted into the terminal window, which is connected to\nexpanse.sdsc.edu. For instructions on how to do this, see the tutorial\non how to use a terminal application and SSH go connect to an SDSC HPC\nsystem: https://github.com/sdsc-hpc-training-org/basic_skills.\n\n\n<a name=\"top\">Contents:\n* [Expanse Overview & Innovative Features](https://github.com/sdsc-hpc-training-org/expanse-101/blob/main/docs/expanse_overview.md)\n* [Getting Started](https://github.com/sdsc-hpc-training-org/expanse-101/blob/main/docs/getting_started.md)\n* [Modules](https://github.com/sdsc-hpc-training-org/expanse-101/blob/main/docs/modules.md)\n* [Account Management](https://github.com/sdsc-hpc-training-org/expanse-101/blob/main/docs/accounts.md)\n* Compiling and Linking Code\n* Running Jobs\n* Hands-on Examples\n* MPI Jobs\n* OpenMP Jobs\n* GPU/CUDA Jobs\n* Hybrid MPI-OpenMP Jobs\n* Data and Storage, Globus Endpoints, Data Movers, Mount Points\n* Final Comments\n\n<hr>\n## Misc Notes/Updates:\n\n* You must have a expanse account in order to access the system.\n * To obtain a trial account:\n http://www.sdsc.edu/support/user_guides/expanse.html#trial_accounts\n* You must be familiar with running basic Unix commands: see the\n following tutorials at:\n * https://github.com/sdsc-hpc-training/basic_skills\n* The ``hostname`` for Expanse is ``expanse.sdsc.edu``\n* The operating system for Expanse is CentOS\n* For information on moving from Comet to Expanse, see the [Comet to Expanse\nTransition Workshpo](https://education.sdsc.edu/training/interactive/202010_comet_to_expanse/index.html)\n\nIf you have any difficulties completing these tasks, please contact SDSC\nConsulting group at help@xsede.org.\n\n","dir":"/expanse-101/","name":"running_jobs_on_expanse.md","path":"expanse-101/running_jobs_on_expanse.md","url":"/expanse-101/running_jobs_on_expanse.html"},{"layout":"default","title":"About the Team","content":"# About the Team\n\n[Mary Thomas](https://www.sdsc.edu/research/researcher_spotlight/thomas_mary.html) is a principal leader of the SDSC HPC Training team.\n\n[James McDougall](http://berserkcomputing.com) is the student intern who worked on the Reverse Proxy Service and documentation.\nCheck out his [github](https://github.com/JamesMcDougallJr). [Email](mailto:jmcdouga@ucsd.edu) him if you have questions about using the reverse proxy service or have questions about Jupyter notebooks.\n\nScott Sakai is the security expert and ops/backend for the Reverse Proxy Service.\n\n[Marty Kandes](https://hpc-students.sdsc.edu/instr_bios/martin_kandes.html) specializes in Singularity containers including Jupyter Notebook containers.\n\n[Bob Sinkovits](https://www.sdsc.edu/research/researcher_spotlight/sinkovits_robert.html) wrote the Python basic skills notebooks.\n","dir":"/notebooks-101/Docs/source/","name":"aboutus.md","path":"notebooks-101/Docs/source/aboutus.md","url":"/notebooks-101/Docs/source/aboutus.html"},{"layout":"default","title":"Contact Us","content":"# Contact Us\n\nIf you have questions or trouble with the material in this tutorial, see the <a href=\"https://comet.sdsc.edu\">Comet User Guide</a>, or please contact the following consulting teams:\n\n<blockquote>\n<table border=\"0\">\n<tr><td><b>XSEDE Help:</b></td> <td>&nbsp;&nbsp;</td> <td>help@xsede.org<td></tr>\n<tr><td><b>Non-XSEDE Help:</b></td><td>&nbsp;&nbsp;</td> <td>consult@sdsc.edu<td></tr>\n</table>\n</blockquote>\n\n\n","dir":"/notebooks-101/Docs/source/","name":"contactus.md","path":"notebooks-101/Docs/source/contactus.md","url":"/notebooks-101/Docs/source/contactus.html"},{"layout":"default","title":"Example Notebooks","content":"# Example Notebooks\nThis page will be updated regularly with example notebooks, primarly for beginners and those who are new to using notebooks on Comet.\n\n## Beginner Tutorials\n* [Python Series](https://github.com/sinkovit/PythonSeries)\n* [Boring Python Notebooks](https://github.com/sdsc-hpc-training-org/notebook_examples/tree/master/Boring_Python)\n* [GPU Notebook Examples](https://github.com/sdsc-hpc-training-org/notebook_examples)\n\n## More Advanced\n* [Data Visualization With Python Using Jupyter Notebooks](https://github.com/sdsc-hpc-training-org/webinars/tree/master/201912_data_viz_python)\n","dir":"/notebooks-101/Docs/source/","name":"examples.md","path":"notebooks-101/Docs/source/examples.md","url":"/notebooks-101/Docs/source/examples.html"},{"layout":"default","title":"Insecurity with direct node access","content":"## Insecurity with direct node access\n\nThis section described how to connection between the browser on your local host (laptop) to a Jupyter service running on Comet over HTTP and demonstrates why the connection is *not* secure.\n\n![connection over HTTP](https://github.com/sdsc-hpc-training-org/notebooks-101/blob/master/Docs/images/jupyter-notebook-http.png?raw=true)\n\n\n\n### Log onto comet.sdsc.edu \n\n`ssh -Y -l <username> <system name>.sdsc.edu`\n\n* create a test directory, or `cd` into one you have already created\n* Clone the examples repository:\n`git clone https://github.com/sdsc-hpc-training-org/notebook-examples.git`\n\n\n### Launch a notebook on the login node:\nRun the jupyter command. Be sure to set the --ip to use the hostname, which will appear in your URL :\n`[mthomas@comet-14-01:~] jupyter notebook --no-browser --ip='/bin/hostname'`\n\nYou will see output similar to below:\n```\n[I 08:06:32.961 NotebookApp] JupyterLab extension loaded from /home/mthomas/miniconda3/lib/python3.7/site-packages/jupyterlab\n[I 08:06:32.961 NotebookApp] JupyterLab application directory is /home/mthomas/miniconda3/share/jupyter/lab\n[I 08:06:33.486 NotebookApp] Serving notebooks from local directory: /home/mthomas\n[I 08:06:33.487 NotebookApp] The Jupyter Notebook is running at:\n[I 08:06:33.487 NotebookApp] http://comet-14-01.sdsc.edu:8888/?token=6d7a48dda7cc1635d6d08f63aa1a696008fa89d8aa84ad2b\n[I 08:06:33.487 NotebookApp] or http://127.0.0.1:8888/?token=6d7a48dda7cc1635d6d08f63aa1a696008fa89d8aa84ad2b\n[I 08:06:33.487 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 08:06:33.494 NotebookApp]\n\n To access the notebook, open this file in a browser:\n file:///home/mthomas/.local/share/jupyter/runtime/nbserver-6614-open.html\n Or copy and paste one of these URLs:\n http://comet-14-01.sdsc.edu:8888/?token=6d7a48dda7cc1635d6d08f63aa1a696008fa89d8aa84ad2b\n or http://127.0.0.1:8888/?token=6d7a48dda7cc1635d6d08f63aa1a696008fa89d8aa84ad2b\n[I 08:06:45.773 NotebookApp] 302 GET /?token=6d7a48dda7cc1635d6d08f63aa1a696008fa89d8aa84ad2b (76.176.117.51) 0.74ms\n[E 08:06:45.925 NotebookApp] Could not open static file ''\n[W 08:06:46.033 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (76.176.117.51) 7.39ms referer=http://comet-14-01.sdsc.edu:8888/tree?token=6d7a48dda7cc1635d6d08f63aa1a696008fa89d8aa84ad2b\n[W 08:06:46.131 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (76.176.117.51) 1.02ms referer=http://comet-14-01.sdsc.edu:8888/tree?token=6d7a48dda7cc1635d6d08f63aa1a696008fa89d8aa84ad2b\n```\n\nNotice that the notebook URL is using HTTP, and when you connect the browser on your local sysetm to this URL, the connection will _not_ be secure. Note: it is against SDSC Comet policy to run applications on the login nodes, and any applications being run will be killed by the system admins. A better way is to run the jobs on an interactive node or on a compute node using the batch queue (see the [Comet User Guide](https://comet.sdsc.edu)), or on a compute node, which is described in the next sections.\n\n### Obtain an interactive node:\nJobs can be run on the cluster in `batch mode` or in `interactive mode`. Batch jobs are performed remotely and without manual intervention. Interactive mode enable you to run/compile your program and environment setup on a compute node dedicated to you. To obtain an interactive node, type:\n`srun --pty --nodes=1 --ntasks-per-node=24 -p compute -t 02:00:00 --wait 0 /bin/bash`\nYou will have to wait for your node to be allocated - which can take a few or many minutes. You will see pending messages like the ones below:\n\n```\nsrun: job 24000544 queued and waiting for resources\nsrun: job 24000544 has been allocated resources\n[mthomas@comet-18-29:~/hpctrain/python/PythonSeries]\n```\n\nYou can also check the status of jobs in the queue system to get an idea of how long you may need to wait.\n\nLaunch the Jupyter Notebook application.\nNote: this application will be running on a compute node, and you must keep track of the given URL:\n\n`jupyter notebook --no-browser --ip='/bin/hostname'`\n\nThis will give you an address which has localhost in it and a token. Something\nlike:\n`http://comet-14-0-4:8888/?token=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`\nYou can then paste it into your browser. You will see a running Jupyter\nnotebook and a listing of the notebooks in your directory. From there everything should be working as a regular notebook.\nNote: This token is your auth so don't email/send it around. It will go away when you stop the notebook.\n\nTo learn about Python, run the ```Python basics.ipynb``` notebook.\nTo see an example of remote visualization, run the ```Matplotlib.ipynb``` notebook!\n\n\n#### Access the node in your browser\nCopy the the URL above into the browser running on your laptop.\n\n#### Use your jupyterlab/jupyter notebook server!\nEnjoy. Note that your notebook is unsecured.\n","dir":"/notebooks-101/Docs/source/methods/","name":"httpConnect.md","path":"notebooks-101/Docs/source/methods/httpConnect.md","url":"/notebooks-101/Docs/source/methods/httpConnect.html"},{"layout":"default","title":"Security with Reverse Proxy Service","content":"## Security with Reverse Proxy Service\n* Connection to Notebook over HTTPS using the [Reverse Proxy Service](https://github.com/sdsc-hpc-training-org/reverse-proxy) (very secure)\n\n### Overview\n\n![arch](https://github.com/sdsc-hpc-training-org/notebooks-101/raw/master/Docs/images/Reverse-Proxy-Service-for-Secure-Jupyter-Notebooks-Arch.png?raw=true)\n\nThe SDSC Reverse Proxy Service is a prototype system that will allow users to launch standard Jupyter Services on on any Comet compute node using a reverse proxy server using a simple bash script called `start-jupyter`. The notebooks will be hosted on the internal cluster network as an HTTP service using standard jupyter commands. The service will then be made available to the user outside of the cluster firewall as an HTTPS connection between the external users web browser and the reverse proxy server. The goal is to minimize software changes for our users while improving the security of user notebooks running on our HPC systems. The RPS service is capable of running on any HPC system capable of supporting the RP server (needs Apache).\n\nUsing the RPS is very simple and requires no tunneling and is secure (produces HTTPS URLs). To use RPS, SSH to connect to comet, and make sure that you have the software environment installed on the login node. Verify that you have installed the required software: `Anaconda`, `conda`, `Jupyter` (notebooks, lab), and other Python packages needed for you application.\n\n### Clone the RPS repository\nClone [this](https://github.com/sdsc-hpc-training-org/reverse-proxy) repository directly into your comet login node. \n`git clone https://github.com/sdsc-hpc-training-org/reverse-proxy.git`\n\n### Launching the Notebook\nThe `start-jupyter` script performs the following tasks:\n* Sends a request to the reverse proxy server (RPS) to get a one-time token and a port number\n* Launches the jupyter notebook command using the token and port number.\n* Prints a secure URL containing the token to the terminal, so that the user can copy/paste the URL into a local browser:\n```\nYour notebook is here:\n https://aversion-runaround-spearman.comet-user-content.sdsc.edu?token=099aa825b1403d58889842ab2c758885\n```\n\n### Usage\n`./start-jupyter [-p <string>] [-d <string>] [-A <string>] [-b <string>] [-t time] [-i]`\n\n```\n\n-p: the partition to wait for. debug or compute\n Default Partition is \"compute\"\n \n-d: the top-level directory of your jupyter notebook\n Default Dir is /home/$USER\n\n-A: the project allocation to be used for this notebook\n Default Allocation is your sbatch system default allocation (also called project or group)\n \n-b: the batch script you want to submit with your notebook. Only those in the `batch` folder are supported.\n Default batch script is ./batch/batch_notebook.sh\n \n-t: the time to run the notebook. Your account will be charged for the time you put here so be careful.\n Default time is 30 minutes\n \n-i: Get extra information about the job you submitted using the script\n\n```\n(If you don't know what $USER is, try this command: `echo $USER`. This is just your comet username)\n\nNote that the time positional argument must occur after all the flags. There will be an error if you put any flags after the positional argument.\n\n**NOTE: Using the script on multiple systems** \n\nThere are minor differences when using the script on Comet vs. Stratus vs. TSCC. TSCC uses a queue system called Torque, whereas Comet and Stratus use Slurm. You will see example notebook and jupyterlab scripts for Torque and Slurm in the RPS repository. The most important thing to notice is that when you run start-jupyter it will automatially run with defaults for the cluster you are using. So you don't need to worry as much about which cluster you're on. \n\n### Example Commands\nStart a notebook with all defaults on any system\n`./start-jupyter`\n\nStart a jupyterlab session with rest defaults on Comet\n`./start-jupyter -b slurm/jupyterlab.sh`\n\nStart a jupyterlab session with rest defaults on TSCC\n`./start-jupyter -b torque/jupyterlab.sh`\n\nStart a notebook in the debug queue on Comet only\n`./start-jupyter -d ~ -p debug -t 30`\n\nStart a notebook in the compute queue on Comet only\n`./start-jupyter -d ~ -A ddp363 -p compute -t 60`\n\n### Example Output\n\nThis is your waiting screen. This screen occurs before your batch job is submitted.\n![Waiting Screen](https://github.com/sdsc-hpc-training-org/reverse-proxy/blob/master/.examples_images/ex1.png?raw=true)\n\nYour notebook is ready to go!\n![Notebook Ready](https://github.com/sdsc-hpc-training-org/reverse-proxy/blob/master/.examples_images/ex2.png?raw=true)\n\nIf you refresh too soon, you may see this page. This is expected and you'll just have to wait.\n![Token Mapping](https://github.com/sdsc-hpc-training-org/reverse-proxy/blob/master/.examples_images/ex3.png?raw=true)","dir":"/notebooks-101/Docs/source/methods/","name":"reverseProxy.md","path":"notebooks-101/Docs/source/methods/reverseProxy.md","url":"/notebooks-101/Docs/source/methods/reverseProxy.html"},{"layout":"default","title":"Security with SSH Tunneling","content":"## Security with SSH Tunneling\nConnection to Notebook over SSH tunneling (secure)\n\n![connection over HTTP](https://github.com/sdsc-hpc-training-org/notebooks-101/blob/master/Docs/images/jupyter-notebook-http-ssh-tunnel.png?raw=true)\n\n\nThis section shows you how to launch a Jupyter Notebook using an interactive node or on a compute node, and to use ssh tunneling to securely connect to the notebook server.\n\n### Interactive Node Method\n#### Open two terminals on your computer\nWe will use one terminal to start the notebook, and the other to establish the tunnel. Pick the first terminal, call it T1.\n\n#### SSH into comet from your local computer:\nIn T1, `ssh user@comet.sdsc.edu`. This is just a regular SSH login.\n\n#### Claim an interactive node\nIn T1, `srun --partition=debug --pty --nodes=1 --ntasks-per-node=24 -t 00:30:00 --wait=0 --export=ALL /bin/bash`\n\n[Source: Comet User Guide](https://www.sdsc.edu/support/user_guides/comet.html)\n\nFeel free to adjust the parameters, but remember that in the debug partition you can only claim a node for up to 30 minutes. You can use other queues, but you may have to wait longer. Take note of the `<node name>` of the interactive node.\n\n#### Start a jupyter notebook server on the interactive node.\nIn T1, run the command\n`jupyter notebook --no-browser`\n\nThe `no browser` option is required, otherwise the program may think you want a text representation of your outputs in the terminal, which trust me - you don't want. You can also specify a port number if you wish using the `--port 1234` option. Note the value of the `<jupyter port>` number returned by the command.\n\n#### Create SSH Tunnel Connection\nIn the next command, you will create an ssh connection between your local host and the notebook port on the remote, interactive node. When you connect your browser to the notebook service, this will channel all communications via the SSH connection, which is secure and encrypted.\nIn the second terminal, call it T2, run the command\n\n`ssh -L 8888:127.0.0.1:<jupyter port> user@comet-14-01.sdsc.edu`\nReplace `comet-14-01` with the name of the compute node. You can view the compute node in T1 prompt. Replace the `<jupyter port>` with the port the jupyter notebook started on after running the `jupyter notebook --no-browser` command in window T1. The default jupyter port number is 8888, but don't worry if its different.\nThis establishes a tunnel between port 8888 on your computer and the jupyter port on the compute node\n\n#### Visit the port in your local browser\nIn any browser, type in 127.0.0.1:8888 and you should get your notebook. You'll have to input the jupyter token available in your terminal.\n\nIf for some reason that address doesn't work, check the output of the terminal. You could try using the address localhost:8888 or 0.0.0.0:8888. The reason tunneling is generally not the prefered method is because it is complicated and the port numbers sometimes are not available by the time you access the service. And, you can't know the port the jupyter notebook is going end up open on until you start it on the node, and you need to tunnel through that port... which is why we need two terminals in this example.\n\n### Compute Node Method\n\nIn this example, we use a batch script to obtain a compute node, and to launch a jupyter lab or notebook. You can access the jupyter service directly from your browser once it has started running on the comet node. This method uses the SSH Tunneling method described above to make a secure SSH connection between your laptop and the Jupyter services.\n\nFirst, log onto comet using SSH.\n\n#### Copy the batch script example\n\n```\n#!/usr/bin/env bash\n#SBATCH --job-name=tensorflow-compute\n#SBATCH --partition=compute\n#SBATCH --time=00:30:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=24\n#SBATCH --output=tensorflow-compute.o%j.%N\nmodule purge\nmodule list\nprintenv\ntime -p singularity exec /share/apps/compute/singularity/images/tensorflow/tensorflow-cpu.simg jupyter lab --no-browser --ip=\"$(hostname)\"\n```\n\nThis example uses the tensorflow singularity container available on comet. You can use any container you want. If you check out `/share/apps/computer/singularity` you can find many useful containers. The key part of this example is how the jupyter lab is started at the end - `jupyter lab --no-browser --ip \"${hostname}\"`.\n\n#### Submit this script to the queue\nSimply run `sbatch run-jupyter-tensorflow-compute.sh`\nOne thing you may want to do is change the script to be `--partition=debug` if you want a shorter wait time.\n\n#### Access the node in your browser\nFirst, wait for the job to be submitted to the queue. Then, monitor the output file created by your batch job, which looks something like `tensorflow-compute.o%j.%N` if you used the example. Inside this file, you will see the output of the jupyterlab command. There, you should be able to see the port the jupyterlab server is running on, as well as the token you will need to login. My recommendation would be to just memorize the port number and copy the jupyter token. The port is almost always 8888 so it shouldn't be that hard to remember. You will also need to know the comet node you are logged in on. You can view this by typing this command: `squeue -u $USER`. Under the NODELIST section you can see the comet node.\n\nOpen up a new tab in your browser, and type in the following: `http://comet-xx-xx.sdsc.edu:PPPP` where `comet-xx-xx` is the comet node, `PPPP` is the port number (usually 8888). The jupyter notebook page should show up, and you can now paste in the token from the output file.\n\n#### Use your jupyterlab/jupyter notebook server!\nEnjoy. Note that your notebook connection is *secured and encrypted*.\n","dir":"/notebooks-101/Docs/source/methods/","name":"tunneling.md","path":"notebooks-101/Docs/source/methods/tunneling.md","url":"/notebooks-101/Docs/source/methods/tunneling.html"},{"layout":"default","title":"Jupyter Notebook Overview","content":"# Jupyter Notebook Overview\n\n![connection over HTTP](https://github.com/sdsc-hpc-training-org/notebooks-101/blob/master/Docs/images/jupyter-notebook-launch-methods.png?raw=true)\n\n\nJupyter Notebooks are interactive web tools known as a computational notebooks, which researchers can use to combine software code, explanatory text and multimedia resources, and computational output, in a single document. Jupyter has emerged as a de facto standard for data scientists and other scientific domains. Notebooks can be launched locally and access local file systems, or they can be launched on a remote machine, which provides access to a user’s files on the remote system. In the latter case, the notebooks are launched via a process that creates a unique URL that is composed of the hostname plus an available port (chosen by the jupyter application) plus a one-time token. The user obtains this URL and enters it into a local web browser, where the notebook is available as long as the process on the remote machine is up and running. By default, these notebooks are not secure, and potentially expose a users local files to unwanted users.\n\nIn this tutorial, we cover SDSC’s multi-tiered approach to running notebooks more securely: running notebooks in the usual way using the insecure HTTP connections; hosting a Jupyter service using HTTPS and Jupyter Lab; and our new Reverse Proxy Service (RPS). When used, the RPS will launch a batch script that creates a securely hosted HTTPS access point for the user, resulting in a safer, more secure notebook environment.\n\n\nBy default, these notebooks are not secure, and potentially expose a user’s local files to unwanted access. In this tutorial, we present SDSC’s multitiered approach to running notebooks more securely.\n","dir":"/notebooks-101/Docs/source/","name":"overview.md","path":"notebooks-101/Docs/source/overview.md","url":"/notebooks-101/Docs/source/overview.html"},{"layout":"default","title":"Software Prerequisites","content":"# Software Prerequisites\n\nRunning Juypter notebooks relies on you handling your own python jupyter package installation. Typically, users install Anaconda on local systems. Anaconda is a common package manager used for data science, but it it not recommended for use on HPC systems and running jupyter notebooks remotely. Anaconda is a large package and has a lot of overhead. For best performance, we recommend using `Miniconda`. \n\n[Miniconda](https://docs.conda.io/en/latest/miniconda.html) is a free minimal installer for conda. It is a small, bootstrap version of Anaconda that includes only conda, Python, the packages they depend on, and a small number of other useful packages.\n\nIf you’re not familiar with Anaconda, check it out [here](https://www.anaconda.com/products/individual).\n\n## Install Miniconda\nTo install Miniconda on Linux, you need to locate and download the installer package for your system. For linx, you will find a list of installers [https://docs.conda.io/en/latest/miniconda.html#linux-installers](https://docs.conda.io/en/latest/miniconda.html#linux-installers). On the HPC system, use:\n```\nwget <link-to-installer-file>\n``` \nto download the install package. For SDSC HPC systems, the current link is the `Miniconda3 Linux 64-bit:` `https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh`\n\nOnce you have downloaded the correct installer, follow the installation instructions [https://conda.io/projects/conda/en/latest/user-guide/install/index.html](https://conda.io/projects/conda/en/latest/user-guide/install/index.html). For SDSC HPC systems Comet, TSCC, and Stratus, the name of the downloaded installer file is `Miniconda3-latest-Linux-x86_64.sh`\n\n### Run the installer\nChange the permissions so you can execute the script: \n```\nchmod +x Miniconda3-latest-Linux-x86_64.sh\n```\nRun the bash install script: \n```\nbash Miniconda3-latest-Linux-x86_64.sh\n``` \nor \n```\n./Miniconda3-latest-Linux-x86_64.sh\n```\nYou should answer yes to almost all of the questions. Make sure to type in the word \"yes\" for the license agreement.\nAlso be sure to type in \"yes\" when the installer asks you if you want to run conda init.\nIn addition, you need to make sure that the installer has placed these two lines into your `.bashrc` file:\n```\nThe Miniconda installer should prompt to add each of the following lines separately to the .bashrc file:\n\n. /home/$USER/miniconda3/etc/profile.d/conda.sh\nconda activate\n```\nIf not present, add the two lines to the file. Once you have done this, restart your bash shell: \nrun the command\n```\nsource ~/.bashrc\n```\nwhich \"restarts\" the shell environment.\n\nMiniconda should now be installed. By default, Miniconda should be installed in your home directory:\n```\nMiniconda3 will now be installed into this location:\n/home/$USER/miniconda3\n```\nIf Miniconda still does not seem to be installed, try using the command `source ~/.bashrc`, which \"restarts\" conda.\n\nTo verify the installation, run the command:\n```\n(base) [mthomas@comet-ln2:~] which conda\n~/miniconda3/bin/conda\n```\n\n## Install Jupyter Notebook\nTo run jupyter notebooks, you need to install the `jupyter` package using the command \n```\nconda install jupyter\n``` \nTo verify the installation, run the command:\n```\n(base) [$USER@comet-ln2:~] which jupyter\n~/miniconda3/bin/jupyter\n```\nMore installation information can be found here: [https://anaconda.org/anaconda/jupyter](https://anaconda.org/anaconda/jupyter).\n\n## Install JupyterLab\nJupyterLab is designed as an extensible environment and can be installed with conda, pip, docker, etc. For full details, see: [https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html](https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html)\n\nTo use `conda` to install jupyerlab, run the terminal command:\n```\nconda install -c conda-forge jupyterlab\n```\nTo verify the installation, run the command:\n```\n(base) [$USER@comet-ln2:~] which jupyter-labextension\n~/miniconda3/bin/jupyter-labextension\n```\n\n\n## Other Python Packages**\nAny other Python packages you need to run your notebook should be installed with Conda. You can install python packages in a conda environment while your notebook is running. This is useful if you forgot a package, you won't have to worry about cancelling and restarting your job before installing. However, it is recommended that you install all required packages beforehand to save yourself valuable compute time.\n\n\n## Setup a Conda Virtual Environment\nChoose whatever name you want - it should reflect the application/project you are working on.\n`$ conda create --name example_env` \n\n### To see which virtual environments you’ve created\n`$ conda env list`\n\nTo use a particular virtual environment (e.g., one named ‘example_env’):\n`$ source activate example_env # Note: don’t use ‘conda activate’`\n\n### To see which versions of a package are available\n`(example_env) $ conda search package_name`\nThis searches for packages from the default “channel.” Other channels might have newer versions available. For instance, we’ve seen more recent versions of the ‘yt’ package in the channel named “conda-forge”. To install from a different channel, use something like:\n`(example_env) $ conda search -c conda-forge yt`\n\n### To install packages in an active virtual environment\n`(example_env) $ conda install package_name # e.g, like ‘yt’`\nAs with the package search, you can install from a different channel using a ‘-c channel_name’ flag, e.g.:\n`(example_env) $ conda install -c conda-forge yt`\n\n### To update a package to a newer version\n`(example_env) $ conda update package_name`\nLike install and search, this command can take a ‘-c channel-name’ flag if you want to update to newer versions than are in the default channel.\n\n### To start a Python interpreter with access to the installed packages:\n`(example_env) $ python # python3 works as well`\n\n### To stop using the current virtual environment:\n`(example_env) $ source deactivate`\n\n### To delete an inactive virtual environment:\n`$ conda env remove --name example_env`\n\n## Other Python Packages\nAny other Python packages you need to run your notebook should be installed with Conda. You can install python packages in a conda environment while your notebook is running. This is useful if you forgot a package, you won't have to worry about cancelling and restarting your job before installing. However, it is recommended that you install all required packages beforehand to save yourself valuable compute time.\n\n## Download Example Notebooks\nFor these examples, you should have some simple notebooks loaded into your comet directory for testing. You can clone the notebooks examples repository:\nTo clone the repo, log onto comet, cd into the directory where you want to work, and type:\n```\ngit clone https://github.com/sdsc-hpc-training-org/notebook-examples.git\n```\n\n## Basic HPC Skills\n\nIf you are a beginner, or need to brush up on some basic skills needed to run jobs on HPC systems, check out our repo:\n\n[Basic Skill](https://github.com/sdsc-hpc-training-org/basic_skills)\n\nTo clone the repo, log onto comet, cd into the directory where you want to work, and type:\n```\ngit clone https://github.com/sdsc-hpc-training-org/basic_skills.git\n```\n","dir":"/notebooks-101/Docs/source/","name":"prerequisites.md","path":"notebooks-101/Docs/source/prerequisites.md","url":"/notebooks-101/Docs/source/prerequisites.html"},{"layout":"default","title":"Jupyter Services on Comet","content":"# Jupyter Services on Comet\n\nOverview of this section\nYou can access a jupyter notebook directly from your browser after starting it on the login node. This method is insecure, and will result in a notebook served over http, which is not something you want to be using on a regular basis.\n\n## Access the node in your browser\nCopy the the URL above into the browser running on your laptop.\n\n### Use your jupyterlab/jupyter notebook server!\nEnjoy. Note that your notebook is unsecured.\n","dir":"/notebooks-101/Docs/source/","name":"runJupyterMethods.md","path":"notebooks-101/Docs/source/runJupyterMethods.md","url":"/notebooks-101/Docs/source/runJupyterMethods.html"},{"permalink":"//","layout":"default","title":"beta-test","content":"# beta-test\nintegrate all notebooks with submodules\n","dir":"/","name":"README.md","path":"README.md","url":"/"}]